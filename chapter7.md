# 第 7 章：自然图像理解与 OCR（含交通牌、扫码、天气等）

## 1. 开篇与学习目标

在多模态大模型（MLLM）的能力版图中，**视觉（Vision）** 是信息带宽最宽的输入模态。对于“驾舱一体”的智能座舱而言，视觉能力不仅意味着让车机“看见”乘客和路况，更意味着要像人类驾驶员一样“理解”复杂的视觉语义。

传统的计算机视觉（CV）往往是孤立的任务（如检测框、分类标签），而 MLLM 的核心价值在于**泛化理解与逻辑推理**。例如，识别出一块路牌是 CV 的任务，但结合当前时间、车辆位置和路牌上的复杂文字（如“工作日7-9点潮汐车道”）来回答“我现在能不能走这条路”，则是 MLLM 的任务。

本章将系统地构建自然图像理解与 OCR 的测评体系，从基础的物体识别到复杂的场景推理，涵盖数据准备、指标设计及车舱落地实战。

**学习目标**：
1.  **构建全栈视觉测评集**：涵盖 General VQA、OCR、Grounding（定位）及特定领域的交通场景数据。
2.  **掌握高阶评价指标**：深入理解 ANLS（OCR 编辑距离）、Hallucination Rate（幻觉率）、POPE（存在性探针）等指标的计算与意义。
3.  **精通鲁棒性测评**：学会设计针对恶劣天气、低光照、运动模糊、远距离小目标（Small Objects）的压力测试。
4.  **理解“端到端”价值**：如何评估 MLLM 在“视觉感知 -> 语义理解 -> 决策建议”全链路中的表现。

---

## 2. 视觉理解能力分层与任务定义

为了科学地测评，我们将 MLLM 的视觉能力划分为四个层级，每一层级对应不同的测试重点：

```ascii
+---------------------------------------------------------------+
| Layer 4: 视觉逻辑推理 (Visual Reasoning)                       |
| ------------------------------------------------------------- |
| 任务: 解释因果、预测未来、常识推理                                |
| 示例: "为什么要减速？" -> "因为前方路面有积冰且前车刹车灯亮了"         |
+---------------------------------------------------------------+
| Layer 3: 细粒度语义与 OCR (Fine-grained Semantics & OCR)       |
| ------------------------------------------------------------- |
| 任务: 场景文字识别、图表理解、仪表盘读取、密集计数                    |
| 示例: 读取路边停车牌细则、识别二维码、读取中控屏报错代码                |
+---------------------------------------------------------------+
| Layer 2: 视觉定位 (Visual Grounding / Referring)              |
| ------------------------------------------------------------- |
| 任务: REC (Referring Expression Comprehension)、REG (Generation) |
| 示例: 输入"红衣服的人在哪?", 输出 BBox [x1,y1,x2,y2]               |
+---------------------------------------------------------------+
| Layer 1: 全局感知与描述 (Global Perception & Captioning)        |
| ------------------------------------------------------------- |
| 任务: 图像标签、粗粒度描述、主要物体识别                           |
| 示例: "这是一张雨天街道的照片"、"前面有一辆车"                      |
+---------------------------------------------------------------+
```

---

## 3. 核心测评领域与数据集选型

### 3.1 通用图像理解 (General VQA)
这是 MLLM 的基线能力。
*   **评测重点**：对图像内容的全面理解，包括颜色、形状、动作、关系。
*   **推荐数据集**：
    *   **MMBench**: 综合性能力评测，采用选择题形式，方便自动化打分，包含逻辑、属性、细粒度感知等。
    *   **MME (Multimodal Evaluation)**: 包含 14 个子任务，特别设计了“Yes/No”类型的指令跟随测试，用于快速回归。
    *   **MMMU**: 侧重于多学科知识（类似考题），虽然对车载不直接相关，但能反映模型的智力上限。

### 3.2 场景文字识别 (Scene Text / OCR)
**车舱核心高频场景**。不同于文档扫描，自然场景下的文字面临变形、遮挡、光照不均等挑战。
*   **评测重点**：
    *   **多语言混合**：中英文混排、特殊符号。
    *   **非规则排版**：竖排（中文店招）、环形文字（Logo）、透视变形（地面的文字）。
    *   **关键信息提取 (KIE)**：不仅读出来，还要结构化（如从发票中提取金额，从路牌提取限速值）。
*   **推荐数据集**：
    *   **TextVQA / ST-VQA**: 针对自然图像中的文字提问。
    *   **OCRBench**: 一个聚合了多个 OCR 任务的综合基准，包含手写、场景、文档等。
    *   **C-TSR (Chinese Traffic Sign Recognition)**: 中文交通标志数据集（需自行构建 VQA 格式对）。
    *   **ICDAR 系列**: 经典的 OCR 竞赛数据集，可用于构建“检测+识别”的端到端测试。

### 3.3 视觉定位 (Grounding)
为了解决“幻觉”问题，要求模型在回答时提供证据（Bounding Box 或 Point）。
*   **评测重点**：模型能否准确指出它所谈论的物体在哪里。这对于“如影随行”（交互式高亮显示）功能至关重要。
*   **推荐数据集**：
    *   **RefCOCO / RefCOCO+ / RefCOCOg**: 给定描述找物体。
    *   **Flickr30k Entities**: 图像描述中的名词短语与 BBox 的对齐。

### 3.4 驾驶垂直领域 (Autonomous Driving Domain)
这是“驾舱一体”的特化测试，要求模型具备驾驶员的常识。
*   **评测重点**：路口理解、交通参与者意图预测、险情识别、Ego-car（自车）行为建议。
*   **推荐数据集**：
    *   **DriveLM / DriveVLM**: 专门构建的驾驶场景 VQA 数据集，包含感知、预测、规划的问答链。
    *   **CODA (Corner Cases in Autonomous Driving)**: 包含大量长尾场景（如路上的动物、散落物、奇形怪状的车），用于测试鲁棒性。
    *   **Mapillary Vistas**: 街景理解，类别极其丰富。

---

## 4. 评价指标体系：从“可比”到“可信”

单纯的 Accuracy 在生成式任务中已经失效。我们需要更细致的量尺。

### 4.1 OCR 专项指标：ANLS
对于 OCR 任务，完全匹配（Exact Match）过于严苛。例如，将 `O` (字母) 识别为 `0` (数字)，或多了个空格，不应判为 0 分。
*   **ANLS (Average Normalized Levenshtein Similarity)**:
    $$ score = 1 - \frac{d(pred, gt)}{\max(|pred|, |gt|)} $$
    其中 $d$ 是编辑距离。如果 $d/max\_len > \tau$ (通常 0.5)，则分数为 0。
    *   **Rule-of-Thumb**: 只要 ANLS > 0.9，通常对人类阅读体验来说差异不可知。

### 4.2 幻觉与安全性指标：POPE & CHAIR
*   **POPE (Polling-based Object Probing Evaluation)**:
    *   询问模型图中是否存在某物体（包含图中有的和图中没有的）。
    *   指标：准确率（Accuracy）、精确率（Precision）、召回率（Recall）。重点关注**阴性预测值**（即图中没有时，模型是否诚实地说“没有”）。
*   **Hallucination Rate (幻觉率)**: 在描述生成的文本中，提及了多少不存在的物体或错误的属性。

### 4.3 定位指标：IoU & Point Hit
*   **IoU (Intersection over Union)**: 预测框与真值框的交并比。通常 IoU > 0.5 视为正确。
*   **Center Point Hit**: 如果应用场景只是“点击”或“高亮”，只要预测点落在真值框内即可算对。

### 4.4 生成式理解打分：LLM-as-a-Judge
对于“描述路况”这种开放问题，使用 GPT-4o 或 Claude-3.5 作为裁判。
*   **Rubric (评分标准)**:
    1.  **准确性**: 是否包含所有关键视觉元素（红灯、行人、雨天）？
    2.  **安全性**: 是否给出了危险的建议（如红灯时建议加速）？
    3.  **逻辑性**: 因果推理是否合理？
*   **打分 prompt 示例**:
    > "请作为自动驾驶安全专家，评估候选回答对路况的描述。重点关注是否存在事实性错误（如看错信号灯颜色）。满分 5 分，错漏关键安全信息直接 0 分。"

---

## 5. 测评工程实施与 Ablation

### 5.1 数据管线与样本构造
1.  **分桶采样 (Bucket Sampling)**：不要只看平均分。按以下维度分桶汇报：
    *   **分辨率**: Low (<256px), Medium, High (>1024px)。
    *   **目标占比**: Small (<1%), Medium, Large (>10%) —— 重点测试 MLLM 是否忽略小目标。
    *   **光照条件**: Day, Night, Dusk/Dawn。
2.  **Prompt 鲁棒性测试**:
    *   同一张图，用 5 种不同的问法（"图里有什么？", "描述这张图", "看到什么了？"），计算输出的一致性。

### 5.2 常见的 Ablation 实验设计
*   **分辨率影响**: 输入 224x224, 448x448, 1024x1024 对 OCR 和小目标检测的影响。通常 OCR 需要高分辨率。
*   **Visual Encoder 选型**: CLIP vs SigLIP vs InternViT。
*   **Token 数量**: 视觉 Token 数量（如 64 vs 256 vs 576）对推理速度和细节感知的权衡。

### 5.3 训练数据反查 (Data Contamination Check)
在打分异常高时，必须反查。
*   **近邻搜索**: 对测试集图片的 Embedding 在训练集中进行向量检索（KNN）。如果余弦相似度 > 0.98，视为潜在的数据泄漏，需剔除该样本。

---

## 6. 常见陷阱与错误 (Gotchas)

1.  **Resizer 的“毁灭性打击”**：
    *   *问题*: 许多 MLLM 默认将图片 Resize 到正方形（如 336x336）。
    *   *后果*: 长条形的路牌或宽幅的全景图被压扁，导致文字不可读或相对位置错乱。
    *   *对策*: 必须使用支持 **任意分辨率 (AnyRes)** 或 **动态切片 (Dynamic Cropping)** 的模型架构或预处理流程。

2.  **OCR 的“脑补”现象**：
    *   *问题*: 模型看到类似 "Starbucks" 的绿色 Logo，即使字迹模糊，也会根据先验知识输出 "Starbucks"。
    *   *后果*: 在识别车牌或验证码等无语义随机字符串时，容易出错。
    *   *对策*: 加入无语义字符（乱码车牌）的测试集，强制模型“所见即所得”，而不是“所见即所想”。

3.  **空间关系混乱**：
    *   *问题*: 问“左边的车是什么颜色？”，模型回答了右边车的颜色。
    *   *对策*: 专门构建 Spatial Reasoning 测试集（Flip 图片后，答案应随之改变）。

---

## 7. 车舱落地：驾舱一体专项

本节讨论如何将上述测评落地到真实的智能座舱产品中。

### 7.1 停车扫码与 POI 发现链路
*   **场景**: 
    1. 用户指着窗外：“那家店评分怎么样？”（POI 识别 + 外部 API 知识）
    2. 进闸口：“扫一下那个码”。（Zoom-in + OCR + 手机联动）
*   **测评设计**:
    *   **E2E 成功率**: `识别意图 -> 截取感兴趣区域(ROI) -> OCR 成功 -> 结构化数据提取` 的全链路转化率。
    *   **长尾测试**: 针对反光玻璃、雨滴附着玻璃、夜间霓虹灯闪烁场景进行专项测试。

### 7.2 仪表盘与中控屏“自检” (Screen-to-Screen)
*   **场景**: 用户问“仪表盘上那个黄色的灯是什么意思？”。
*   **特殊性**: 输入是车内摄像头的画面，拍摄车内屏幕。
*   **测评难点**: 摩尔纹（Moiré pattern）干扰。需构建包含屏幕拍摄画面的特定数据集。

### 7.3 天气与环境感知的“置信度边界”
*   **场景**: 视觉感知到“路面湿滑”，建议切换驾驶模式。
*   **安全原则**: **宁可漏报，不可误报**。
*   **指标设计**: 
    *   **误报率 (False Positive Rate)**: 在晴天误报雨雪的代价很高（频繁骚扰用户）。需设定极低的 FPR 阈值（如 < 1%）。
    *   **校准误差 (Calibration Error)**: 模型输出的置信度（Confidence Score）应与实际准确率线性对应。

### 7.4 驾舱多模态融合案例：交通指挥手势
*   **高阶任务**: 识别交警的手势（停止、直行、靠边停车）。
*   **多模态输入**: 往往需要结合**视频流**（时序动作）而不仅是单帧图像。
*   **测试集**: 必须包含中国国标交警手势库，以及不同角度（侧面、背面）的交警数据。

---

## 8. 练习题

### 基础题
1.  **数据集分类**: 请将以下数据集归类为“OCR”、“Grounding”或“General VQA”：`TextVQA`, `RefCOCO`, `MMBench`, `C-TSR`.
2.  **指标计算**: 目标字符串是 "Parking"，模型输出 "Parkng"。请计算其归一化编辑距离（Levenshtein Distance）及 ANLS 分数。
3.  **Prompt 设计**: 为“交通标志识别”任务设计一个 Chain-of-Thought (CoT) Prompt，引导模型先描述形状颜色，再读取文字，最后推断含义。

### 挑战题
4.  **系统设计**: 某车型计划上线“路书生成”功能，即 MLLM 自动拍摄沿途风景并生成游记。请设计一套评测方案，评估其生成的“美学质量”和“内容真实性”。
    *   *Hint: 涉及由粗到细的筛选，以及对于幻觉（地名匹配）的校验。*
5.  **失败分析**: 模型在识别红绿灯时，经常将路边的红灯笼误识别为红灯。请提出 3 种改进数据或 Prompt 的策略，并设计验证实验。
    *   *Hint: 负样本挖掘、Grounding 约束、逻辑校验。*
6.  **端侧部署**: 车机端侧算力有限，只能运行 2B 参数量的 MLLM。如何设计一套“云端大模型 + 端侧小模型”的协同测评方案？
    *   *Hint: 区分实时性要求高（端侧）和知识性要求高（云端）的任务路由准确性。*

<details>
<summary>点击查看练习题参考方向</summary>

1.  **分类**:
    *   TextVQA: OCR
    *   RefCOCO: Grounding
    *   MMBench: General VQA
    *   C-TSR: OCR (交通垂类)
2.  **计算**:
    *   "Parking" (len 7) vs "Parkng" (len 6).
    *   编辑距离 = 1 (少了一个 'i').
    *   Max Len = 7.
    *   ANLS = 1 - (1/7) ≈ 0.857.
3.  **CoT Prompt**:
    *   "请分析图中的交通标志。第一步：描述标志的形状（圆形/三角形/方形）和底色；第二步：提取标志中的所有文字和数字；第三步：结合形状和文字，解释该标志的具体交通规则含义。"
4.  **路书测评**:
    *   美学：使用专门的美学评分模型（如 NIMA）或人工打分（构图、清晰度）。
    *   真实性：提取生成文本中的地名/POI，与 GPS 记录的 POI 列表做 IOU 匹配。
5.  **红灯笼误检**:
    *   策略1（数据）：在训练集中加入大量包含红灯笼、红色霓虹灯的“负样本”。
    *   策略2（Prompt）："请找出图中的圆形红色发光体，并判断其是否有灯杆支撑和遮光罩，确认是否为交通信号灯。"
    *   策略3（Grounding）：强制模型输出 BBox，如果 BBox 位于路边店铺而非路口上方，则过滤。
6.  **端云协同**:
    *   测评核心是 **Router (路由)** 的准确性。
    *   数据集包含两部分：A类（急需响应，如“这是红灯吗”），B类（闲聊/查询，如“这朵花叫什么”）。
    *   指标：A类路由到端的比例（需接近100%），B类路由到云的比例，以及端侧模型的 Latency 和云侧模型的 Richness。

</details>
