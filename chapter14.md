# 第 14 章：代码生成能力评测（作为逻辑性与 Agent 能力 Proxy）

## 1. 开篇：代码——逻辑的终极形式

在 MLLM（多模态大模型）的评测体系中，**代码生成（Code Generation）** 往往被狭隘地理解为“辅助程序员编程”的垂直功能。然而，在通用的智能体（Agent）视角下，代码生成具有远超其字面意义的战略价值。

代码是**逻辑的终极形式**。与自然语言的歧义性不同，代码具有严格的语法约束、确定的执行路径和不可妥协的因果关系。如果一个模型无法生成逻辑严密、可执行的代码，它就很难在复杂的 Agent 场景中进行长链路规划、工具调用或数学推理。

本章将重新定义代码评测：它不仅是软件工程能力的度量，更是模型**逻辑推理（Reasoning）**、**指令遵循（Instruction Following）** 以及 **外部世界交互（Tool Use）** 能力的最佳**代理指标（Proxy Metric）**。

**本章学习目标**：
1.  **理解代理机制**：为何代码生成能力强弱直接映射了 Agent 的 ReAct 规划与工具调用水平。
2.  **掌握评测全貌**：从单元测试（Unit Test）到仓库级（Repo-level）理解，再到沙箱执行（Sandbox Execution）的全链路设计。
3.  **精通指标体系**：深入理解 `pass@k` 的统计学原理及其在避免“运气成分”中的作用。
4.  **驾舱一体实战**：学习如何在车机端侧环境评测 DSL（领域特定语言）生成、API 编排与自动化配置脚本的安全性。

---

## 2. 核心论点：为什么代码是 Agent 能力的强 Proxy？

### 2.1 文本推理 vs. 代码推理 (CoT vs. PoT)
在解决复杂逻辑问题（如数学应用题、时序规划）时，传统的 **思维链（Chain-of-Thought, CoT）** 使用自然语言进行推理。然而，大模型容易出现计算错误或逻辑跳跃。
**程序思维（Program-of-Thought, PoT）** 则要求模型将推理过程转化为代码（通常是 Python）。
*   **评测意义**：如果模型能写出正确的求解方程代码，说明它不仅“懂”了题意，还具备了形式化建模的能力。代码评测实际上是在测模型的**形式化建模（Formal Modeling）** 能力。

### 2.2 工具调用即函数生成
在 Agent 语境下，模型调用一个外部工具（如“查询天气”或“控制车窗”），本质上就是生成一段**函数调用代码**（Function Call）。
*   **映射关系**：
    *   API 文档 $\approx$ 函数定义与 Docstring。
    *   用户意图 $\approx$ 需求描述。
    *   Tool Argument Filling $\approx$ 代码参数赋值。
*   **结论**：一个连 Python 函数参数都填不对的模型，绝对无法成为一个可靠的 Agent。

### 2.3 逻辑闭环的唯一性
自然语言的回答往往难以自动验证真伪（需要昂贵的 Model-based Judge），而代码的验证成本极低且极其客观——**编译器不骗人**。代码评测提供了大规模、低成本、高置信度的自动化逻辑评估手段。

---

## 3. 评测基准与数据集地图

为了全面评估，我们需要构建分级的评测数据集：

### 3.1 L1: 算法与功能级（Functional Correctness）
*   **任务**：输入 Docstring 或问题描述，输出函数体。
*   **经典基准**：
    *   **HumanEval / MBPP**：事实上的工业标准。主要测试基础 Python 语法和标准库使用。
    *   **HumanEval+ / MBPP+**：增强了测试用例的基准，防止模型“背题”或针对性过拟合。
*   **关键点**：必须使用**Canonical Solutions**（标准答案）来生成测试用例，确保题目本身无误。

### 3.2 L2: 数据科学与库使用（Data Science & Libraries）
*   **任务**：使用 Pandas, Matplotlib, NumPy 等第三方库处理数据。
*   **基准参考**：DS-1000, PandasEval。
*   **评测价值**：这是 MLLM 进行“文档对话”、“表格分析”能力的底层支撑。主要考察模型对**API 生态**的熟练度。

### 3.3 L3: 仓库级与长上下文（Repo-level & Context）
*   **任务**：给定一个完整的 GitHub 仓库，要求修复 Bug 或添加新 Feature。
*   **基准参考**：SWE-bench。
*   **评测价值**：考察 **跨文件推理（Cross-file Reasoning）**、依赖分析以及在长上下文（Long Context）中定位关键定义的能力。

### 3.4 L4: 多模态转代码（Visual-to-Code）
*   **任务**：输入一张 UI 截图或图表，输出复现该 UI 的 HTML/CSS 或绘图代码。
*   **评测价值**：视觉理解与代码生成的融合，详见第 16 章，但其核心执行引擎评测属于本章范畴。

---

## 4. 评测系统架构与方法论

### 4.1 静态分析 (Static Analysis) —— 快速筛选
在不运行代码的情况下进行检查。
*   **AST 解析**：检查代码是否存在语法错误（Syntax Error）。
*   **Linter 检查**：使用 Flake8/Pylint 检查未定义变量、未使用的导入等。
*   **局限性**：无法检测逻辑错误（Logic Error），如算法写反了但语法是对的。仅作为“冒烟测试”。

### 4.2 动态执行沙箱 (Dynamic Execution Sandbox) —— 核心引擎
这是代码评测的“心脏”。必须构建一个**高隔离、无状态**的执行环境。

```text
[ 评测控制器 (Controller) ]
       | 分发任务
       v
[ 容器池 (Docker/gVisor Pool) ]
   +---------------------------+
   | 沙箱实例 (Instance)       |
   |  [ 代码注入 (Injector) ]  |---> 写入 generated_code.py
   |  [ 测试运行器 (Runner) ]  |---> 运行 pytest / unittests
   |  [ 资源监视器 (Monitor) ] |---> 监控 CPU/RAM/Time
   +---------------------------+
       | 返回结果 (Exit Code, Stdout, Stderr)
       v
[ 结果聚合与清洗 (Aggregator) ]
```

*   **隔离技术**：推荐使用 gVisor 或 Firecracker microVM，比标准 Docker 更安全，防止恶意代码逃逸。
*   **网络策略**：默认**断网**（除非评测 Agent 联网能力），防止模型试图从互联网下载答案或攻击内网。
*   **资源熔断**：
    *   **Time Limit**：防止 `while True` 死循环。
    *   **Memory Limit**：防止内存泄漏或恶意占用。
    *   **Disk Limit**：防止填满磁盘。

### 4.3 核心指标：深入 Pass@k
单纯的准确率（Accuracy）在生成式任务中是有误导性的。我们使用 `pass@k`。

*   **定义**：针对同一 Prompt，采样生成 $n$ 个代码样本 ($n \ge k$)，计算其中至少有一个样本通过所有单元测试的概率。
*   **无偏估计公式**：
    $$ \text{pass}@k = 1 - \frac{\binom{n-c}{k}}{\binom{n}{k}} $$
    其中 $n$ 是采样总数，$c$ 是通过测试的样本数。
*   **解读**：
    *   **Pass@1**：代表模型“一次做对”的能力，反映了模型的**置信度与精确性**（适合低延迟场景）。
    *   **Pass@10**：代表模型的**潜在上限**，即在有外部重试机制或人工辅助筛选下的能力（适合辅助编程或离线 Agent 规划）。

---

## 5. Ablation 与 归因分析

当代码评测分数下降时，需要通过 Ablation 实验定位原因：

1.  **指令遵循 vs. 编码能力**：
    *   模型是没看懂需求（Instruction Following 差），还是看懂了但写不对算法（Coding 差）？
    *   *测试方法*：提供伪代码或详细步骤，看分数是否提升。

2.  **上下文依赖**：
    *   代码是否因为找不到依赖库而失败？
    *   *测试方法*：在 Prompt 中显式提供 `import` 语句或环境描述。

3.  **格式遵循 (Formatting)**：
    *   模型是否在代码块中夹杂了 Markdown 说明导致解析失败？
    *   *测试方法*：优化后处理（Post-processing）正则提取逻辑，对比 Raw 输出。

---

## 6. 车舱落地：驾舱一体中的代码生成

在智能座舱中，代码生成不是为了写 App，而是为了实现**极致的自动化与个性化**。这是一类特殊的 **DSL（Domain Specific Language）** 生成任务。

### 6.1 典型场景评测设计

1.  **自然语言转车控 DSL**：
    *   *场景*：“如果后座有人且温度高于 28 度，就打开后排空调并播放轻音乐。”
    *   *评测难点*：逻辑嵌套（If-Then）、多模态状态感知（DMS 信号）、多域控制（空调+媒体）。
    *   *构建基准*：定义一套虚拟的车辆控制 API（如 `Vehicle.Zone.Rear.AC.set(On)`），评测模型生成调用链的准确性。
    *   *Metric*：**AST Match Rate**（生成的抽象语法树与真值是否一致），比纯文本匹配更鲁棒。

2.  **复杂查询构造 (Query Generation)**：
    *   *场景*：“找一下这附近 5 公里内评分 4.5 以上且现在营业的火锅店。”
    *   *任务*：模型需要生成针对地图/POI 引擎的结构化查询对象（JSON 或 SQL-like）。
    *   *评测*：Mock 地图服务，检查查询参数（Filter 条件、Sort 键值、Range 限制）的正确性。

3.  **Ambiguity Resolution (歧义消除)**：
    *   *场景*：用户说“打开窗户”。
    *   *任务*：模型生成的代码不应直接操作，而应包含“查询当前窗户状态”->“判断哪扇窗”->“操作”或“反问”的逻辑。
    *   *评测*：**Defensive Coding Score**（防御性编程得分）。

### 6.2 端侧安全与沙箱 (Safety Guardrails)

车载环境对代码执行有极高的安全红线。

*   **只读沙箱 (Read-Only Logic)**：
    *   评测模型是否能在“模拟模式”下运行。即代码生成只产生“Plan”，而不直接执行“Action”。
    *   *Checklist*：生成的代码必须返回一个 Action List，而不是直接调用底层硬件驱动。

*   **资源预算 (Compute Budget)**：
    *   车载芯片（如高通 8295）的 CPU 资源宝贵。
    *   *评测指标*：**Token Efficiency**（完成任务所需的代码 token 数）和 **Cyclomatic Complexity**（圈复杂度）。过于复杂的生成代码应被扣分，因为这会增加端侧推理和执行的延迟。

*   **死循环与阻塞检测**：
    *   严格评测模型是否会生成阻塞主线程的代码（如 `time.sleep()` 在 UI 线程）。
    *   *测试集*：对抗性 Prompt（如“一直等到温度下降”），看模型是否生成 `while` 循环（Bad）还是注册回调事件（Good）。

---

## 7. 本章小结

*   **Code is Logic**：代码评测是衡量 MLLM 逻辑推理和形式化建模能力的“硬通货”。
*   **Dynamic is King**：必须建立基于沙箱的动态执行环境，静态文本相似度指标在代码评测中基本无效。
*   **Agent 基础**：Tool Use 本质上是 Function Calling 代码生成。高 `pass@k` 的代码模型是强 Agent 的必要条件。
*   **车载特殊性**：驾舱一体场景下，代码评测侧重于 DSL 转换准确率、API 参数填充正确性以及端侧执行的安全性与低时延。

---

## 8. 练习题

### 基础题 (50%)

1.  **概念理解**：解释为什么在代码评测中，`Pass@1` 比 `Accuracy`（准确率）更科学？
    *   *Hint：考虑大模型生成的随机性以及单一采样的偶然性。*
2.  **指标计算**：如果在 10 次采样中，有 2 次通过了测试，计算 `pass@1` 的无偏估计值。
    *   *Hint：套用 $1 - \frac{n-c}{n}$ 的简化思路（当 k=1 时）。*
3.  **沙箱机制**：简述为什么不能直接在评测服务器的宿主机 Shell 中运行模型生成的代码？列举两个具体风险。
    *   *Hint：`rm -rf`, 环境变量泄露，挖矿脚本。*

### 挑战题 (50%)

4.  **评测设计（Agent 方向）**：设计一个评测任务，测试模型使用“计算器工具”和“日历工具”解决问题的能力。输入是“我出生在 1990 年 5 月 1 日，今天我活了多少天？”。
    *   *Hint：模型需要生成两段代码/调用：1. 获取今天日期；2. 日期减法计算。如何评测这两步的依赖关系？*
5.  **车舱场景分析**：在车机端，用户指令是“把空调调得像春天一样”。模型生成了一段代码将温度设为 22 度，风速设为柔和。如何设计客观指标来评价这段代码的“合理性”而不是“唯一正确性”？
    *   *Hint：引入 Fuzzy Match（模糊匹配）或 Range Check（范围检查），而不是 Exact Match。*
6.  **Fail Case 分析**：模型生成的代码在 Python 3.10 上运行通过，但在 Python 3.6 上报错。这属于什么类型的错误？在工程化评测中如何规避？
    *   *Hint：环境依赖错误。Docker 镜像版本锁定。*

<details>
<summary>点击查看答案解析</summary>

1.  **Pass@1 vs Accuracy**：Accuracy 通常指贪婪解码（Greedy）下的单次结果，但代码生成往往需要探索（Temperature > 0）。Pass@1 是统计意义上的期望，且通过无偏估计公式可以纠正采样方差。
2.  **计算**：当 k=1 时，公式简化为 $c/n$ 的期望。在无偏估计公式中，pass@1 = $1 - \frac{8}{10} = 0.2$ (20%)。
3.  **风险**：1. 文件系统破坏（删除关键数据）；2. 网络攻击（将宿主机作为跳板攻击内网）；3. 资源耗尽（Fork 炸弹）。
4.  **Agent 设计**：评测重点在于 **依赖链（Dependency Chain）**。如果第一步（获取日期）错了，第二步逻辑再对也是错。评测应支持 Mock 第一步的返回，单独测试第二步的逻辑；同时也测试端到端正确性。
5.  **合理性评测**：定义一个“春天参数集”真值范围（如 Temp $\in [20, 24]$, Wind $\in [Low, Medium]$）。只要生成的参数落在此区间内即判为 Pass。
6.  **环境问题**：这是**兼容性/版本依赖错误**。对策：评测基准必须明确指定 Python 版本和依赖库版本（requirements.txt），并在 Dockerfile 中固化。
</details>

---

## 9. 常见陷阱与错误 (Gotchas)

1.  **Eval Harness 的隐藏 Bug**：
    *   *陷阱*：评测框架自身的测试用例写错了，或者测试用例太弱（比如只测了 `add(1,1)=2`，没测 `add(-1, -1)`），导致模型虚高。
    *   *对策*：定期人工抽检 Passed 的样本，使用 Mutation Testing（变异测试）评估测试用例的质量。

2.  **Prompt 泄露与过拟合**：
    *   *陷阱*：模型输出了 HumanEval 题目中独有的变量名（如 `bf` 代表 `brute_force`），但 Prompt 里没给。说明模型训练数据污染。
    *   *对策*：使用 **Decontamination**（去污染）流程，或者使用类似 LiveCodeBench 这种基于最新 GitHub 代码构建的动态基准。

3.  **非确定性行为 (Nondeterminism)**：
    *   *陷阱*：涉及 `dict.keys()` 遍历或集合操作的代码，在不同 Python 版本或哈希种子下顺序不同，导致测试失败。
    *   *对策*：在测试断言中，对列表输出先 `sort()` 再比较，或使用集合比较。

4.  **车机 API 的幻觉**：
    *   *陷阱*：模型生造了一个不存在的 API，如 `Vehicle.set_driver_mood("happy")`。
    *   *对策*：引入 **Linter for DSL**。在执行前，先用 API Schema 校验生成的代码，所有函数名和参数必须在白名单内。
