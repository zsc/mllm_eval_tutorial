# 第 1 章：测评总览与能力树

## 1.1 开篇：绘制多模态能力的“地图”

欢迎进入 MLLM（多模态大语言模型）测评的系统化教程。

在传统的 NLP 时代，测评主要围绕“文本进，文本出”进行。然而，当模型进化为 MLLM 时，我们面临的是一个**输入输出组合呈指数级爆炸**的复杂系统。一个合格的 MLLM，不仅要是一个“作家”（文本生成），还要是“观察者”（视觉理解）、“倾听者”（语音识别）、“程序员”（代码/动作生成）甚至是“决策者”（Agent）。

测评 MLLM 的核心难点不在于跑几个脚本，而在于**定义“什么是好”**。
- 一张路牌图片，OCR 识别对了文字，但没理解“禁止左转”的红圈含义，算对还是错？
- 用户说“把那个关掉”，模型需要结合视线追踪（DMS）才知道用户看的是车窗还是空调，这该怎么测？

**本章学习目标：**
1.  **能力解构**：学会区分“原子能力”与“系统能力”，构建全维度的 I/O 矩阵。
2.  **测评层级**：理解从感知、逻辑、安全到体验的“测评金字塔”。
3.  **基准设计**：掌握如何选择 Baseline（SOTA、竞品、上一版本、人类基线）。
4.  **交付标准化**：定义 Model Card、Leaderboard 和回归报告的标准格式。
5.  **车舱落地**：建立“驾舱一体”场景下的测评全景图（端云结合、多模交互）。

---

## 1.2 MLLM 的“能力”到底指什么？

我们在讨论测评时，经常混淆两个概念：**模型做题的能力**（Academic Benchmarks）与**模型解决问题的能力**（Product Capabilities）。

### 1.2.1 原子任务能力 (Atomic Capabilities)
这是指模型在单一模态、单一任务定义下的表现。通常对应学术界的公开数据集。
*   **示例**：ASR 的字错误率 (WER)、图像分类的 Top-1 Accuracy、OCR 的字符准确率。
*   **特点**：客观、易量化、易复现。
*   **局限**：高分不一定高能。ASR 听得很准，但意图理解错了，用户体验依然是 0 分。

### 1.2.2 系统综合能力 (System Capabilities)
这是指模型作为 Agent，串联多个原子能力完成用户意图的表现。
*   **示例**：用户指着中控屏上的导航地图问：“这附近哪家咖啡厅能在 5 分钟内走到？”
    *   需要能力链条：ASR（听懂） -> Intent（意图识别） -> Tool Use（调用地图 API 获取 POI） -> Logic（筛选 < 5min 步行距离） -> TTS（语音回答）。
*   **特点**：主观性强、链路长、错误会累积。

> **Rule of Thumb #1: 木桶效应与乘法效应**
>
> 在 MLLM 测评中，系统能力的成功率近似于链路上各原子能力成功率的**乘积**。
> 如果 ASR 90%，意图理解 90%，API 调用 90%，最终系统成功率只有 $0.9^3 \approx 72.9\%$。
> **测评策略**：原子测评用于**归因分析**（哪里断了），端到端测评用于**验收交付**（能不能用）。

---

## 1.3 多模态输入输出矩阵 (I/O Matrix)

为了不遗漏测评死角，我们需要建立一个全排列的矩阵。每一行代表输入，每一列代表输出。

```ascii
+-------------------+---------------------------------------------------------------+
| 输入模态 (Source) |                      输出能力与关键测评点 (Target)             |
+-------------------+---------------------------------------------------------------+
|                   | -> [Text]  Captioning, VQA, OCR(文档/自然场景), 幻觉检测       |
| 1. Image / Video  | -> [Box]   Grounding (坐标定位), 检测与分割, 计数              |
| (视觉感知)        | -> [Code]  GUI 截图转 HTML/Python, 图表转数据 (De-plot)        |
|                   | -> [Risk]  敏感内容过滤 (涉黄/暴/隐私)                         |
+-------------------+---------------------------------------------------------------+
|                   | -> [Text]  ASR (长/短/流式), 歌词对齐, 说话人分离 (Diarization)|
| 2. Audio / Music  | -> [Attr]  情绪识别, 性别/年龄识别, 语种识别 (LID)             |
| (听觉感知)        | -> [Event] 环境声分类 (警笛/婴儿哭/玻璃碎 - 车载重点)          |
|                   | -> [Music] 风格分析, 乐理结构理解                              |
+-------------------+---------------------------------------------------------------+
|                   | -> [Text]  逻辑推理, 摘要, 翻译, 角色扮演 (Role-play)          |
| 3. Text           | -> [Speech] TTS (自然度, 情感可控, 声音克隆相似度)             |
| (认知核心)        | -> [Image] 文生图 (一致性, 美学评分, 文字渲染能力)             |
|                   | -> [Code]  代码生成, SQL 生成, 单元测试通过率                  |
|                   | -> [Action] Tool/API Call (JSON 格式正确性, 参数准确性)        |
+-------------------+---------------------------------------------------------------+
| 4. Multimodal Mix | -> [E2E]   视频+语音问答, 屏幕+指令操作, 驾驶场景险情描述      |
| (多模态融合)      | -> [State] 记忆更新 (用户偏好画像), 上下文状态保持             |
+-------------------+---------------------------------------------------------------+
```

**重点关注的“隐形”模态：**
*   **Time (时间)**：视频和音频都有时序。模型能否理解“先...后...”、“视频第 3 秒发生了什么”。
*   **Space (空间)**：3D 理解。模型能否理解图片中的遮挡关系、距离远近（尤其是单目视觉下的深度估计）。

---

## 1.4 测评金字塔：分层评价体系

不要试图用一个分数概括模型。对于车舱等严肃场景，必须采用分层门禁（Gating）策略。

```ascii
             / \
            /   \        Level 4: 体验与拟人 (Experience)
           /-----\       --------------------------------
          /       \      关键词：有趣、风格一致、多轮不烦、共情
         /---------\     测评法：Elo Rating, 众包 Side-by-Side, 细粒度 Rubric
        /           \
       /-------------\   Level 3: 安全与鲁棒 (Safety & Robustness)
      /               \  -----------------------------------------
     /-----------------\ 关键词：拒答攻击、指令注入、防甚至幻觉、隐私合规
    /                   \ 测评法：Red Teaming (红队测试), 边界值压力测试
   /---------------------\
  | Level 2: 逻辑与执行   | Level 2: 逻辑与执行 (Reasoning & Agency)
  | (Logic & Execution)   | -----------------------------------------
  |                       | 关键词：多步推理、RAG 事实性、代码逻辑、工具调用
  +-----------------------+ 测评法：GSM8K类, IFEval, 客观正确率 (Exact Match)
  | Level 1: 基础感知     | Level 1: 基础感知 (Perception)
  | (Perception)          | -----------------------------------------
  +-----------------------+ 关键词：听清(WER)、看准(mAP/IoU)、读对(OCR)
                            测评法：标准学术数据集, 自动化脚本
  =========================
  [ 底座约束: 时延 (Latency) | 吞吐 (Throughput) | 显存成本 (VRAM) ]
```

### 1.4.1 分层执行策略
*   **Level 1 & 3 (Hard Gate)**：如果是车载模型，ASR 听不清指令，或者安全测试不通过（如允许用户语音打开引擎盖），**直接不准上线**。这两个层级适合做 CI/CD 的自动化阻断。
*   **Level 2 (Soft Gate)**：逻辑能力通常随模型规模提升。需要设定“基线分”，低于基线需特批。
*   **Level 4 (Monitoring)**：体验往往是千人千面的。通常不作为阻断项，而是作为 A/B 测试的观察指标。

---

## 1.5 评测资产与交付物

不要只给老板发一个 Excel 表格。专业的测评体系需要维护以下资产：

### 1.5.1 Model Card (模型身份证)
每当发布一个新模型版本，必须附带 Model Card，包含：
*   **训练数据截止日期**：防止询问“昨天的新闻”产生幻觉。
*   **能力边界**：明确写出“不能做”的事（例如：本模型不支持识别手写潦草字体）。
*   **适用场景**：推荐用于闲聊助手，不推荐用于医疗诊断。

### 1.5.2 Leaderboard & Regression Dashboard
*   **纵向对比（回归）**：v1.2 vs v1.1。必须计算 **Win/Tie/Loss** 比例。
    *   *警惕*：总分提升，但核心能力下降。例如模型变得更聪明了，但 OCR 变差了（遗忘灾难）。
*   **横向对比（竞品）**：Compare with GPT-4o, Gemini 1.5 Pro, Qwen2-VL, etc.
    *   *意义*：确立当前模型的行业水位。

### 1.5.3 Bad Case Taxonomy (错误归因库)
不仅要记录错题，还要对错误进行分类（Taxonomy）：
1.  **感知错误**：根本没看清图/没听清音。
2.  **知识缺失**：看清了，但不知道那是什么（例如不认识某种新型路标）。
3.  **逻辑断裂**：推理步骤跳跃。
4.  **指令遵循失败**：没按要求格式输出（如要求输出 JSON 却输出了 Markdown）。
5.  **拒答/过度防御**：本来能答的，因为安全策略误判而拒绝。

---

## 1.6 基线与对照：如何选 Baseline

> **Rule of Thumb #2: 测评的本质是“相对论”**
> 绝对分数（如 85.4 分）没有意义，分数的**差值**（Delta）才有意义。

在设计实验时，至少包含以下三类 Baseline：

1.  **SOTA (State-of-the-Art) 上限**：
    *   通常选择最强的闭源模型（如 GPT-4o）。这是用来打击自信心和寻找差距的。
2.  **Previous Best (自研历史最佳)**：
    *   这是用来证明“我这周没白干”的。必须通过回归测试。
3.  **Heuristic / Pipeline (非端到端基线)**：
    *   *重要*：对于 RAG 或工具调用，如果不比“关键词检索 + 规则”更好，那为什么要用昂贵的 LLM？
    *   对于 OCR，如果不比传统的 PP-OCR 或 Tesseract 更好，那 MLLM 还有什么价值？

---

## 1.7 车舱落地：驾舱一体测评概览

本教程的特色在于每章末尾的“上车”环节。车舱环境是 MLLM 最复杂、最苛刻的落地场景之一。

### 1.7.1 驾舱一体交互链路图

在车里，MLLM 不是一个孤立的 App，它是连接人与车的**中枢神经**。

```ascii
     [人: 驾驶员/乘客]        [环境: 路况/天气]       [云端: 知识/服务]
            |                        |                       |
+-----------v------------------------v-----------------------v-----------+
|                          输入层 (Sense)                                |
| 1. 舱内视觉 (IMS): DMS(疲劳/视线), OMS(手势/物品), Lip-reading(唇语)   |
| 2. 舱内听觉: 麦克风阵列 (声源定位, 降噪, AEC回声消除)                  |
| 3. 车辆状态: 车速, 档位, 剩余里程, 故障码                              |
| 4. 舱外感知: ADAS数据 (车道线, 障碍物), 环视摄像头, 天气               |
+------------------------------------+-----------------------------------+
                                     |
+------------------------------------v-----------------------------------+
|                          中枢层 (Think)                                |
| 1. 仲裁 (Arbitration): 谁在说话？该听谁的？(副驾调音量 vs 主驾语音导航)|
| 2. 路由 (Routing): 端侧处理(车控/隐私) vs 云端处理(闲聊/百科)          |
| 3. 记忆 (Memory): 长期偏好(空调习惯) + 短期上下文(刚才提到了那家店)    |
| 4. 安全 (Safety): 驾驶分心检测, 危险指令拦截                           |
+------------------------------------+-----------------------------------+
                                     |
+------------------------------------v-----------------------------------+
|                          输出层 (Act)                                  |
| 1. 语音 (TTS): 分区播放, 情感安抚, 紧急打断                            |
| 2. 视觉 (GUI): 中控屏卡片, AR-HUD 投射, 仪表盘提示                     |
| 3. 车控 (Control): 空调, 车窗, 座椅, 氛围灯, 导航设点                  |
+------------------------------------------------------------------------+
```

### 1.7.2 车载测评的核心挑战（相比通用测评）

1.  **端侧 vs 云侧的割裂**：
    *   车机芯片算力有限（NPU）。我们需要测评“量化后的 7B/3B 模型”在端侧的表现，对比云端全精度模型的**性能折损率**。
    *   **断网测评**：在无网络情况下，基础车控语音助手必须可用。

2.  **多模态融合触发 (Multimodal Triggering)**：
    *   *场景*：用户看向右后视镜（DMS） + 指着右边（手势） + 说“把他打开”（语音）。
    *   *测评点*：模型能否对齐这三个模态的时间戳，正确识别出“打开右后视镜折叠”或“打开右侧车窗”的意图？

3.  **高噪声与远场环境**：
    *   通用 ASR 测评通常在安静环境下。
    *   **车载必测**：高速风噪（120km/h）、胎噪、车内音乐播放时的“Barge-in”（打断唤醒）、多人同时说话（Cocktail Party Effect）。

4.  **安全边界 (Safety Boundary)**：
    *   *红线*：模型绝不能在行驶中通过语音指令执行涉及行车安全的操作（如“挂P档”、“打开后备箱”、“关闭大灯”），除非有二次确认或速度限制。
    *   测评中需要包含大量的**“钓鱼指令”**来测试模型的拒绝能力。

---

## 1.8 本章小结

*   **全维覆盖**：MLLM 测评必须建立覆盖 图/文/声/视/控 的全 I/O 矩阵。
*   **分层治理**：底层测感知（CI 门禁），中层测逻辑（SFT 优化目标），顶层测体验（人工验收）。
*   **基准思维**：永远要有 Baseline，没有对比的指标没有价值。
*   **场景为王**：在车舱场景下，时延、鲁棒性（抗噪）、安全边界和端侧能力比单纯的“高智商”更重要。

---

## 1.9 练习题

### 基础题
1.  **分类题**：将以下任务归类到测评金字塔的层级（Level 1-4）。
    *   (A) 准确识别出图片中红绿灯的颜色。
    *   (B) 用户说“我很累”，模型用温柔的语气讲个笑话并调暗灯光。
    *   (C) 拒绝用户“帮我伪造一张请假条”的请求。
    *   (D) 根据用户模糊的描述“带我去那个有大恐龙的商场”，推断出目的地。

2.  **判断题**：在车载场景下，如果一个云端大模型回答准确率 99%，但平均延迟 5 秒，是否可以替代准确率 90% 但延迟 0.5 秒的端侧模型进行车控操作（如开窗）？为什么？

3.  **场景题**：请列举出 3 种 MLLM 的输入模态和 3 种输出模态。

<details>
<summary>点击查看基础题提示与答案</summary>

**答案：**
1.  (A) Level 1 感知; (B) Level 4 体验; (C) Level 3 安全; (D) Level 2 逻辑。
2.  **不可以**。车控操作（开窗、调温）属于强实时交互，用户对物理反馈的预期极快。5秒延迟会导致用户重复指令或认为系统故障，造成严重体验下降甚至分心。高频、低风险、强实时的操作应优先端侧。
3.  输入：图像、音频、文本（或视频、3D点云）；输出：文本、语音、代码（或 Action/JSON、图像）。
</details>

### 挑战题
4.  **设计题**：你要测评一个“车载儿童陪伴助手”（部署在后排屏幕）。
    *   你会设计哪些特殊的**多模态**输入 Case？
    *   你需要关注哪些特殊的**安全指标**？
    *   如何评估其**长期记忆**能力？

5.  **思考题**：为什么说“代码生成能力”是 GUI Agent（屏幕操作助手）能力的 Proxy（代理指标）？如果模型不会写代码，它能做好 GUI 操作吗？

<details>
<summary>点击查看挑战题提示与答案</summary>

**答案：**
4.  **输入 Case**：小孩的哭闹声（音频）、小孩拿着绘本对着摄像头（视觉OCR+TTS朗读）、小孩语无伦次的童言童语（ASR容错）。**安全指标**：内容过滤（恐怖/色情/暴力）、隐私保护（不收集儿童人脸ID）、引导向善（不教坏小孩）。**长期记忆**：隔天能否记得小孩的名字、喜欢的动画片角色。
5.  **原因**：代码生成要求极高的逻辑严密性、符号对应能力和规划能力。GUI 操作本质上是将自然语言转化为结构化的 API 调用或 DOM 树操作序列，这在思维链路上与写代码高度同构。如果模型连 Python 的缩进和变量名都搞不定，很难指望它能准确地在复杂的 UI 树中定位元素并执行多步操作。
</details>

---

## 1.10 常见陷阱与错误 (Gotchas)

*   **陷阱 1：只测“静态”，不测“动态”**。
    *   测评只用了静态图片做 VQA，而忽略了视频流。结果模型上线后，面对 continuously changing 的驾驶画面，无法理解“刚才那辆车去哪了”。
*   **陷阱 2：过度依赖 GPT-4 作为 Judge**。
    *   虽然 LLM-as-a-Judge 很流行，但 GPT-4 也有偏见（喜欢长回复、喜欢某种格式）。在车舱这种短指令场景下，GPT-4 可能会把简洁准确的回答打低分。
    *   *对策*：必须有人工标注的“黄金集”来校准自动打分器的相关性。
*   **陷阱 3：忽视 Prompt Engineering 对测评的影响**。
    *   很多时候模型测出来分低，是因为测评脚本里的 Prompt 写得太烂，或者没有遵循模型的 Chat Template。
    *   *对策*：测评应使用模型官方推荐的最佳 Prompt 模板，或者进行 Few-shot 引导。
