# 第 5 章：TTS 测评（语音合成）

## 1. 开篇：从“能听清”到“全双工拟人交互”

在 MLLM（多模态大模型）时代，语音合成（TTS）的角色发生了质的飞跃。传统的 TTS 往往是一个独立的后端模块，仅负责“文本转语音”。而 MLLM 时代的 TTS（或 Speech Generation）往往与 LLM 深度融合，甚至是端到端（Speech-to-Speech）模型的一部分。

这意味着测评不再局限于**“字正腔圆”**，而是扩展到了**“像人一样交流”**。模型需要根据上下文自动调整语气（开心、遗憾、讽刺），能够处理非言语声音（叹气、笑声、停顿），甚至需要支持流式打断。

**本章学习目标**：
1.  **全链路指标体系**：掌握从信号质量、内容准确性到表现力的分层测评体系。
2.  **工业级稳定性测试**：学会检测 MLLM 特有的“语音幻觉”、无限复读、甚至怪叫等恶性样本。
3.  **主观评测工程化**：如何设计科学的 MOS/MUSHRA 实验，减少人员偏差。
4.  **车舱场景落地**：深入理解车载环境下的声学挑战、管道优先级管理与多音区评测。

---

## 2. 任务拆解与能力矩阵

我们将 TTS 能力划分为四个层级，每一层级对应不同的测评重点。

```ascii
+---------------------------------------------------------------+
|                    TTS Capability Pyramid                     |
+---------------------------------------------------------------+
| [L4] 拟人交互层 (Human-like Interaction)                      |
|      - 情感演绎、非言语表达(笑/叹)、副语言(填充词)、口语化   |
+---------------------------------------------------------------+
| [L3] 风格与场景层 (Style & Context)                           |
|      - 角色扮演(Roleplay)、长文本一致性、多语言Code-Switch    |
+---------------------------------------------------------------+
| [L2] 语言准确层 (Linguistic Accuracy)                         |
|      - 多音字(Polyphone)、文本归一化(TN)、韵律停顿(Prosody)   |
+---------------------------------------------------------------+
| [L1] 基础声学层 (Acoustic Quality)                            |
|      - 清晰度、无底噪、无爆音、音色还原度、采样率             |
+---------------------------------------------------------------+
```

### 2.1 核心任务类型
1.  **标准播报 (Reading)**: 新闻、有声书。要求：稳定、不累、发音极准。
2.  **对话交互 (Chat)**: 语音助手。要求：低延迟、语气自然、短句表现力。
3.  **情感/风格迁移 (Style Transfer)**: "用海绵宝宝的声音读财报"。要求：音色相似度、情感强度。
4.  **流式生成 (Streaming)**: 边想边说。要求：首包延迟 (TTFA)、抗网络抖动。

---

## 3. 测评数据集与语料准备

仅仅有一堆文本是不够的。你需要构建结构化的测试集（Test Suite）。

### 3.1 开源数据集参考（用于微调或对比基准）
| 数据集 | 语言 | 特点 | 用途 |
| :--- | :--- | :--- | :--- |
| **AISHELL-3** | 中文 | 多说话人、高保真 | 基础发音与多音色基准 |
| **LibriTTS-R** | 英文 | 文本对齐精准、去噪处理 | 英文基准、ASR辅助测评 |
| **ESD / EmoV-DB** | 中/英 | 包含明确情感标签 | 情感控制能力测评 |
| **WenetSpeech** | 中文 | 包含大量真实场景噪声 | 用于训练抗噪性或作为负样本参考 |
| **CSMSC (Baker)** | 中文 | 单一女声、标注极细 | 韵律与停顿的标准参考 |

### 3.2 自建“金标准”测试集策略
工业界必须维护一套**Hard Case**集合（回归测试集）：
1.  **TN 专项 (Text Normalization)**:
    *   *数值混合*: "长3m，重5kg"（米/千克 vs 艾姆/凯记）。
    *   *电话与年份*: "1998年" vs "尾号1998"。
    *   *算式*: "3/4"（四分之三）。
2.  **多音字陷阱**: "银行行长"、"给予"、"校对"。
3.  **专有名词**: 当地地名（"六安"读 lù ān）、车企品牌、最新网络热词。
4.  **长文本压力**: 超过 500 字的连续生成，检测音色是否会在后半段发生漂移（Timbre Drift）或甚至退化为杂音。

> **Rule of Thumb**: **测试集必须动态更新**。每当线上用户反馈一个读错的 Case，必须立即加入回归测试集。

---

## 4. 客观测评体系 (Objective Evaluation)

客观指标旨在实现低成本、大规模的自动化监控。

### 4.1 内容准确性 (Intelligibility)
利用强大的 ASR 模型（如 Whisper-large-v3 或 商业 API）作为“判官”。

*   **WER / CER (Word/Character Error Rate)**:
    *   公式：$ \frac{S + D + I}{N} $ (替换+删除+插入 / 总字数)。
    *   *注意*：需要先对 TTS 输入文本和 ASR 输出文本做**文本归一化**（去除标点、统一数字格式）后再比对。
*   **TN 准确率 (Text Normalization Accuracy)**:
    *   针对数字、日期的专项 CER。例如 TTS 输入 "1月1日"，ASR 输出 "一月一日"，算正确；ASR 输出 "一月一号"，算语义正确但字面错误（需根据业务宽容度定义）。

### 4.2 声纹与音质 (Timbre & Quality)
*   **SECS (Speaker Embedding Cosine Similarity)**:
    *   提取生成音频与参考音频的声纹向量（如使用 ResNet-34 based d-vector 或 ECAPA-TDNN），计算余弦相似度。
    *   *阈值*: 通常 > 0.75 或 0.8 认为音色相似。
*   **MCD (Mel-Cepstral Distortion)**:
    *   计算谱距离。**注意**：仅适用于“非自回归”且与 Reference 严格对齐的场景。对于 MLLM 这种生成式、韵律变化大的模型，MCD **不再可靠**，建议弃用或仅作参考。
*   **UTMOS / MOSNet**:
    *   使用神经网络预测 MOS 分。虽然不如人耳准确，但可以用来筛选极差的样本（比如 MOSNet预测分 < 2.0 的样本往往有严重底噪或静音）。

### 4.3 稳定性与瑕疵检测 (Stability & Glitch)
MLLM 生成音频最怕“发疯”。

*   **VAD 占比 (Voice Activity Detection)**:
    *   音频总长 10s，VAD 检测人声只有 1s -> 可能是**静音故障**。
    *   音频总长 10s，VAD 检测人声 9.9s -> 可能是**缺乏停顿**或底噪过大。
*   **WER 爆炸检测**:
    *   如果某条样本 WER > 80%，通常不是发音不准，而是模型输出了**乱语 (Babble)** 或 **复读 (Repetition)**。
*   **信号级检测**:
    *   **削波率 (Clipping Rate)**: 统计幅值达到最大值（如 32767）的采样点比例。
    *   **能量方差**: 检测是否有异常的忽大忽小。

### 4.4 性能指标 (Performance)
*   **RTF (Real Time Factor)**: $\frac{\text{生成耗时}}{\text{音频时长}}$。需 $< 1.0$ (离线) 或 $< 0.5$ (流式)。
*   **TTFA (Time to First Audio)**: 首包延迟。流式 TTS 的核心指标。
    *   *优秀*: < 200ms (接近人类反应)
    *   *及格*: < 500ms
    *   *差*: > 1s (用户会以为没反应)

---

## 5. 主观测评工程 (Subjective Evaluation)

人耳是最终的尺度。

### 5.1 评测协议选择
1.  **MOS (Mean Opinion Score)**: 绝对打分 (1-5)。
    *   *适用*: 评估单个模型的整体质量。
    *   *缺点*: 标注员标准不一，方差大。
2.  **CMOS (Comparison MOS) / SBS (Side-by-Side)**: 相对打分 (-3 到 +3)。
    *   *适用*: 模型迭代（V1 vs V2）。"B比A好多少？"
    *   *优点*: 对微小改进更敏感。
3.  **MUSHRA**: 带锚点（Anchor）的多样本盲测。
    *   *配置*: Reference (无损录音), Anchor (低通滤波/加噪), Model A, Model B...
    *   *适用*: 高保真音质的精细对比。

### 5.2 问卷维度设计 (The Rubric)
不要只问“好不好听”，要拆解：
*   **自然度 (Naturalness)**: 像真人吗？有机械感吗？
*   **韵律感 (Prosody)**: 停顿、重音是否符合语义逻辑？
*   **清晰度 (Articulation)**: 有没有吞音、含糊不清？
*   **情感符合度 (Emotion Match)**: (给定提示词“愤怒”) 这个声音听起来愤怒吗？
*   **音质 (Sound Quality)**: 有没有电流声、爆破音？

---

## 6. 车舱落地：驾舱一体专项评测

车载环境是 TTS 最复杂的应用场景，涉及听觉与视觉的融合、多声源竞争以及安全合规。

### 6.1 车内声学对抗测试 (In-Cabin Acoustic Robustness)
在实验室无法完全模拟车内体验，必须引入环境仿真或实车测试。

*   **噪声叠加测试**:
    *   采集不同工况噪音：*胎噪 (60/120kmh)*、*风噪 (开窗)*、*空调最大档*、*雨刮器声*。
    *   将 TTS 输出与噪声按不同信噪比 (SNR) 混合，再进行 ASR 识别或人耳听测。
    *   **目标**: 在 60km/h 工况下，TTS 的可懂度不应显著下降（可能需要 TTS 自动触发**谱增强**或**响度补偿**）。

### 6.2 交互链路评测 (Pipeline & Interaction)
*   **Barge-in (打断) 性能**:
    *   TTS 正在播报时，用户说话打断。
    *   *指标*: **AEC (回声消除) 残留率**。如果 TTS 的声音回流到了麦克风被识别成指令（Self-trigger），是严重 Bug。
*   **Ducking (压低) 曲线**:
    *   当 TTS 介入时，背景音乐/收音机应平滑降低音量。
    *   *测评点*: 压低是否突兀（Hard cut）？恢复是否自然？TTS 音量是否始终比背景音高出 6-12dB？

### 6.3 多音区与空间音频 (Multi-Zone & Spatial)
*   **独立音区隔离度**:
    *   驾驶位播报导航，副驾是否觉得吵？后排看视频，声音是否窜入驾驶位头枕音响？
    *   *测评*: 使用假人录音设备（HATS）在不同座位录音，计算**串音衰减 (Crosstalk Attenuation)**。
*   **定向播报**:
    *   "请向左转" -> 声音是否通过声场算法营造出从左侧发出的听感？

### 6.4 离线与云端混合 (Hybrid Mode)
*   **无缝切换**:
    *   车辆驶入隧道（断网），云端 TTS 失败，切为端侧 TTS。
    *   *测评点*: 
        1. **音色一致性**: 端侧小模型音色是否与云端大模型差异过大？
        2. **拼接痕迹**: 切换点是否有爆音或重复？
        3. **功能降级**: 端侧 TTS 可能不支持复杂的 "角色扮演"，是否能平稳回退到标准播报？

### 6.5 安全策略
*   **紧急打断**: 遇到 ADAS（辅助驾驶）紧急告警（如前车急刹），TTS 必须在 <50ms 内被切断或压低，让位给报警音。这是**红线指标**。

---

## 7. 常见陷阱与调试技巧 (Gotchas)

| 陷阱 (Pitfall) | 表现 | 调试/解决方案 |
| :--- | :--- | :--- |
| **采样率欺骗** | 看起来是 48k 音频，实际上 12k 以上全是空的。 | 查看 **Spectrogram (语谱图)**。如果高频部分骤然截止（Cut-off），说明经过了上采样，音质虚高。 |
| **Text Normalization 悖论** | 只有数字读错了，其他都对。 | 不要指望端到端模型能完美处理所有数字。**必须**在前端挂载基于规则的 TN 处理器（如 WeTextProcessing），哪怕是 LLM 也不如 Regex 稳定。 |
| **神经声码器伪影** | 偶尔出现短暂的金属音或水泡声。 | 这通常是 GAN-based Vocoder 的训练不稳定性。增加判别器训练步数，或在推理时引入 HiFi-GAN 的去噪后处理。 |
| **首字吞音** | 句子的第一个字经常听不清或很轻。 | 检查流式推理的 Warm-up。往往是因为 Padding 不够，导致第一个 Mel 帧生成不完整。 |
| **过度拟人 (Over-acting)** | 导航播报时带有叹气或过度情绪，让司机烦躁。 | 对“功能性”场景（导航、控车）强制约束情感强度，或使用专门的 Neutral 风格模型。 |

---

## 8. 本章小结

TTS 测评已经从单一的信号处理问题，演变成了**语义理解+信号处理+交互体验**的综合学科。
1.  **数据要广**：一定要包含日期、算式、多音字等 Corner Case。
2.  **客观要准**：利用 ASR 和声纹识别做大规模回归，但要知道它们的边界。
3.  **主观要细**：区分音质、自然度、情感三个维度的打分。
4.  **上车要严**：在噪杂、多任务并发的车机环境中，稳定性与可懂度优于花哨的情感表达。

---

## 9. 练习题

<details>
<summary><strong>基础题 1：ASR 评分的局限性 (点击展开)</strong></summary>

**Q: 使用 ASR 计算 CER 来评估 TTS 时，如果发现 CER 很高，是否一定说明 TTS 效果差？**

> **Hint**: 思考 ASR 本身的错误率以及多音字、方言的影响。
>
> **答案**: 不一定。
> 1. ASR 模型本身可能有误识别（尤其是生僻词、专有名词）。
> 2. TTS 可能发音正确但带有口音或情感色彩（如哭腔），导致标准 ASR 识别失败。
> 3. 文本归一化不一致（如 TTS 读 "二零二三"，ASR 输出 "2023"），若未做预处理会导致 CER 虚高。
</details>

<details>
<summary><strong>基础题 2：RTF 计算 (点击展开)</strong></summary>

**Q: 某 TTS 系统生成一段 5 秒的音频，在 GPU 上耗时 0.5 秒。请问其 RTF 是多少？如果切换到 CPU 推理耗时 6 秒，此时 RTF 是多少？哪个能满足流式需求？**

> **Hint**: RTF = 处理时间 / 音频时长。流式要求 RTF < 1。
>
> **答案**:
> GPU RTF = 0.5 / 5 = 0.1。
> CPU RTF = 6 / 5 = 1.2。
> 只有 GPU 场景满足流式需求（RTF < 1），CPU 场景下生成速度跟不上播放速度，会产生卡顿（Buffer Underrun）。
</details>

<details>
<summary><strong>挑战题 3：车机多音区干扰设计 (点击展开)</strong></summary>

**Q: 设计一个实验，评估驾驶位 TTS 播报对后排乘客电话会议的干扰程度。**

> **Hint**: 涉及声压级 (SPL)、串音衰减、主观干扰评分。
>
> **答案**:
> 1. **设置**: 驾驶位播放 TTS 导航（标准音量，如 70dB SPL），后排乘客位置放置 HATS (人工头) 模拟通话。
> 2. **测量**: 记录后排位置的声压级。计算 **Zone Isolation (音区隔离度)** = 驾驶位SPL - 后排SPL。
> 3. **干扰测试**: 在后排录音中混入 TTS 漏音，测试语音通话算法（如 Teams/Zoom 的降噪）是否能滤除漏音，或是否导致通话断续。
> 4. **主观评分**: 邀请测试员在后排进行模拟通话，对“是否听清对方”和“是否被前排打扰”进行 1-5 打分。
</details>

<details>
<summary><strong>挑战题 4：情感 TTS 的“恐怖谷”检测 (点击展开)</strong></summary>

**Q: MLLM 有时会生成极度夸张甚至诡异的笑声或哭声（恐怖谷效应）。如何设计一个自动化指标来预警这种情况？**

> **Hint**: 异常的能量分布、基频 (F0) 抖动范围、情感分类置信度。
>
> **答案**:
> 1. **F0 范围检测**: 统计音频的基频范围。如果 F0 变化率过大或超出人类正常发声范围（如瞬间跳变 2 个八度），标记为异常。
> 2. **情感强度分类器**: 训练一个回归模型预测“情感强度”。如果强度 > 阈值（如极度歇斯底里），则结合语义检查是否合适。
> 3. **非言语检出**: 使用专门的 Audio Event Detection 模型检测“尖叫”、“怪笑”等类别，如果 Prompt 只是普通对话而检测到尖叫，则是严重 Bug。
</details>
