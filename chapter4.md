# 第 4 章：ASR 测评（语音识别）

## 4.1 开篇段落与学习目标

在多模态大模型（MLLM）的交互链路中，ASR（Automatic Speech Recognition）扮演着“听觉感知层”的角色。对于传统的语音助手，ASR 只是将声音转为文字；但对于 MLLM，ASR 是 **Prompt 的注入接口**。ASR 的微小错误（如丢失否定词、数字听错、实体名混淆）经过 LLM 的推理放大，往往会导致严重的逻辑幻觉或任务执行失败。

本章不只关注“听写准确率”，更关注“语义保真度”与“端到端体验”。我们将深入探讨如何建立一个覆盖多场景、多语种、多噪声环境的自动化测评体系。

**本章学习目标**：
1.  **构建全维度的指标体系**：从基础的 WER/CER 到语义一致性（Semantic WER）、流式稳定性（Flicker）及时间戳准确度。
2.  **掌握数据合成与增强策略**：如何利用 RIR（房间冲击响应）和噪声库构建“车舱级”高难度测试集。
3.  **实施归一化工程**：解决 ASR 测评中最大的痛点——文本格式对齐（Normalization）。
4.  **车舱场景深度实战**：驾舱一体下的回声消除（AEC）、多音区仲裁、离线/云端混合链路测评。

---

## 4.2 ASR 任务拆解与输入输出

在 MLLM 语境下，ASR 任务不再单一。根据输入音频流的特性和下游需求，需拆解为不同子任务进行独立测评。

### 4.2.1 任务分类矩阵

| 任务类型 | 输入特征 | 输出要求 | 典型场景 | 测评重点 |
| :--- | :--- | :--- | :--- | :--- |
| **短指令 (Command)** | < 5秒，意图单一 | 文本，需精准匹配 | “打开空调”、“导航去公司” | **句级准确率 (SA)**，拒识率 |
| **长音频 (Long-form)** | > 30秒，甚至数小时 | 文本 + 标点 + 段落 | 会议纪要、视频字幕生成 | **WER/CER**，标点召回率，幻觉率 |
| **流式 (Streaming)** | 实时音频流 (Chunked) | 增量文本 (Partial) | 实时对话、同声传译 | **延迟 (Latency)**，结果修正抖动 |
| **富信息 (Rich Meta)** | 多人对话、复杂背景 | 文本 + 说话人ID + 时间戳 | 多人会议、车内闲聊 | **DER (分离错误率)**，时间戳偏差 |

### 4.2.2 测评流水线架构 (ASCII)

```text
               [测试集: Clean / Noisy / Far-field]
                             ||
                             \/
[音频流模拟器] --> (可选: 物理回放/数字加噪) --> [待测模型/API]
                                                      ||
                                            [原始输出 (Raw Hypothesis)]
                                                      ||
[真值 (Ground Truth)]                              [后处理 (Post-proc)]
        |                                             |
[ITN: 逆文本归一化] <--------- (格式对齐) --------> [ITN: 逆文本归一化]
(例: "一百二十" -> "120")                         (例: "120" -> "120")
        |                                             |
        +------------------> [打分器] <---------------+
                                |
             +------------------+------------------+
             |                  |                  |
      [WER/CER计算]      [延迟/稳定性统计]    [Badcase 聚类分析]
```

---

## 4.3 数据集选型与合成策略

不要只信任模型在公开数据集上的跑分（那是训练集的近邻）。**Rule of Thumb**：自建测试集的价值远高于公开数据集。

### 4.3.1 开源数据集地图

建议按 **L1 (基础能力)** -> **L2 (复杂场景)** -> **L3 (极限挑战)** 分级构建：

*   **L1 基础集（基线校准）**：
    *   **AISHELL-1** (中文): 录音棚环境，过分干净。用于冒烟测试，CER 应 < 2%。
    *   **LibriSpeech-Test-Clean** (英文): 有声书。用于验证英文基础模型未退化。
*   **L2 真实场景（核心指标）**：
    *   **WenetSpeech (Test_Net)**: 互联网视频音频，含口音、背景音、语速变化。**最推荐的中文泛化性测试集**。
    *   **GigaSpeech**: 英文播客/YouTube，覆盖真实口语（犹豫、吞音）。
    *   **Common Voice**: 多语言众包数据，口音极度丰富。
*   **L3 极限挑战（抗噪与远场）**：
    *   **CHiME 系列**: 专门针对嘈杂环境（咖啡厅、街道、家庭）。
    *   **AISHELL-4**: 会议室场景，重叠语音（Overlap）。

### 4.3.2 “车舱级”合成数据配方

车舱环境获取真值成本高，必须掌握合成技术（Data Augmentation for Eval）：

1.  **声源 (Source)**: 选取干净的指令集（TTS 生成或录音棚录制）。
2.  **噪声库 (Noise)**:
    *   *平稳噪声*: 胎噪（低频）、风噪（中高频）、引擎声。
    *   *非平稳噪声*: 鸣笛、雨声、开关门声、旁人干扰声。
3.  **空间响应 (RIR)**:
    *   使用声学仿真软件或实地录制的 **Impulse Response (冲击响应)**。
    *   卷积操作：`Noisy_Audio = (Source * RIR) + (Noise * SNR_Scale)`
4.  **测试矩阵构建**:
    *   SNR: 0dB, 5dB, 10dB, 20dB
    *   速度: 0km/h (静止), 60km/h (市区), 120km/h (高速)
    *   车窗: 关/开
    *   **产出物**: 针对每个模型版本，输出一张 `WER Heatmap`（热力图），横轴为噪音声级，纵轴为指令类型。

---

## 4.4 指标体系深解

### 4.4.1 准确性：超越 WER

*   **WER/CER (Word/Character Error Rate)**
    *   公式：$ \frac{S + D + I}{N} \times 100\% $
    *   **S (Substitution)**: 替换（如 "拨打" -> "波打"）。
    *   **D (Deletion)**: 漏字（如 "打开空调" -> "打开"）。
    *   **I (Insertion)**: 多字（如 "导航" -> "导航啊"）。
    *   *注意*：对于中文，通常使用 CER；对于英文及代码混杂，使用 WER。
*   **Keyword Spotting Rate (KWS)**
    *   针对指令（如“调整温度到25度”），只关注“温度”和“25”是否正确。
    *   **Entity-WER**: 仅计算命名实体（地点、人名、歌曲名）的错误率。这对车载导航至关重要。
*   **Semantic Accuracy (语义准确率)**
    *   利用 LLM 作为 Judge。
    *   Prompt: *“文本A是‘我想听周杰伦的歌’，文本B是‘我想听周结伦的歌’。请判断B是否保留了A的核心意图？”*
    *   解决同音字导致的 WER 虚高问题。

### 4.4.2 延迟 (Latency) 与 体验

*   **RTF (Real Time Factor)**: $\frac{\text{处理耗时}}{\text{音频时长}}$。离线模型需 RTF < 0.3 才能保证体验。
*   **First Token Latency (FTL)**: 用户说完第一个字，到屏幕显示第一个字的时间差。
*   **Final Latency**: VAD 判定用户说完，到整句最终结果定格的时间。
*   **Flicker Rate (闪烁率)**:
    *   流式识别中，后续结果修正前面已显示结果的频率。
    *   高 Flicker 会让用户眼花缭乱，体验极差。
    *   指标：平均每句修正字符数 / 句子长度。

### 4.4.3 稳定性与鲁棒性

*   **Hallucination Rate (幻觉率)**: 在静音段（VAD 未截断时）模型输出无意义文本（如“谢谢观看”、“字幕组”）的概率。Whisper 类模型常见问题。
*   **Time-Stamp Drift**: 词级时间戳与真实时间的平均偏差（ms）。这对口型同步（Lip-sync）至关重要。

---

## 4.5 工程化：文本归一化 (Text Normalization)

**这是 ASR 测评中最大的“坑”。** 90% 的“模型错误”其实是“格式不匹配”。

### 4.5.1 归一化流水线 (Normalization Pipeline)

在计算 WER 之前，**必须**对 Truth 和 Hypothesis 同时执行以下操作：

1.  **全半角转换**: 统一将 `，。！？` 转为半角或全角，或直接移除标点（如果只测文字）。
2.  **数字转写 (ITN)**:
    *   规则：`一千二百` -> `1200`；`两点半` -> `2:30`。
    *   工具：推荐使用 `WeTextProcessing` 或 `Nemo` 的 ITN 模块。
3.  **语气词过滤**: 移除 `嗯`、`啊`、`呃` 等（除非评测对象是逐字记录）。
4.  **英文大小写**: 统一转小写。
5.  **空格处理**: 中文汉字间去空格，英文单词间留单空格。

### 4.5.2 模糊匹配 (Fuzzy Matching)

对于车机特定术语，建立同义词表（Alias List）：
*   Hypothesis: "打开 **氛围** 灯"
*   Truth: "打开 **气氛** 灯"
*   如果在同义词表中 `氛围 == 气氛`，则判定 S=0。

---

## 4.6 错误分析与训练数据反查

### 4.6.1 错误分类学 (Taxonomy)

| 错误类型 | 现象描述 | 可能原因 | 解决方向 |
| :--- | :--- | :--- | :--- |
| **截断 (Cut-off)** | 句首/句尾丢字 | VAD 阈值太高，Endpointing 太激进 | 调整 VAD 参数，增加 padding |
| **拼接 (Merge)** | 两句话粘连，中间无标点 | VAD 未切分，LM 偏向长句 | 检查解码器最大长度惩罚 |
| **热词丢失** | 专名（如“小鹏”）识别错 | 语言模型中该词概率低 | 增加 Hotword boosting / Contextual biasing |
| **幻觉** | 纯静音输出文本 | 训练数据中存在未清洗的字幕 | 增加静音样本训练，惩罚重复生成 |

### 4.6.2 Ablation 实验设计

*   **Context Ablation**: 调整送入 MLLM 的音频 Context 长度，观察是“听得更准”还是“幻觉更多”。
*   **Prompt Engineering**: 针对 Whisper 等模型，测试不同 Prompt（如 "以下是关于汽车导航的指令"）对 WER 的影响。
*   **Beam Size**: 测试 Beam=1 (Greedy) vs Beam=5 的精度与延时 Trade-off。

---

## 4.7 车舱落地：驾舱一体专项测评

车舱环境具有**强噪声、多声源、弱网、高安全要求**的特征。

### 4.7.1 必测场景清单

1.  **AEC (Acoustic Echo Cancellation) 回声消除**:
    *   **测试法**: 车机以 30%, 60%, 80% 音量播放音乐/新闻，人声发出指令。
    *   **关键指标**: **WERR (WER Reduction)**。即 `(无AEC错误率 - 有AEC错误率) / 无AEC错误率`。
    *   **Barge-in (打断) 成功率**: 在播报 TTS 时，用户插话打断的召回率。
2.  **多音区锁定与仲裁 (Multi-zone & Arbitration)**:
    *   **测试配置**: 4麦/6麦阵列录制。
    *   **干扰测试**: 主驾说指令，副驾/后排同时闲聊。模型应只识别主驾（如果主驾唤醒）。
    *   **串音漏报率**: 别人说话，系统误以为是主驾指令的概率。
3.  **离线/在线混合链路 (Hybrid ASR)**:
    *   **一致性测评**: 同样的音频，分别送入离线引擎和云端引擎。
    *   **Diff 分析**: 离线模型通常参数小，需重点评估其在 **泛化指令** 上的退化程度。如果离线听不懂，是否能正确拒识（Reject）而不是乱猜。
4.  **端到端时延预算**:
    *   从 VAD End 到 NLU 意图输出，车规级要求通常 < **500ms**。ASR 部分通常只有 200-300ms 预算。

---

## 4.8 本章小结

ASR 测评是 MLLM 语音交互的基石。一个优秀的测评体系不仅要算得准 WER，还要能模拟真实世界的糟糕声学环境，并能通过语义指标和归一化手段还原模型的真实能力。对于车舱场景，**AEC 性能、多音区抗干扰、以及极速的端侧响应**是评测的三大核心支柱。

---

## 4.9 练习题

### 基础题
1.  **WER 计算实战**:
    *   Ref: "play music please"
    *   Hyp: "play magic"
    *   **Hint**: 计算 S, D, I。注意 token 数量。
    *   <details><summary>答案</summary>Ref 长度 3。Hyp 长度 2。"music" -> "magic" (S=1), "please" -> deleted (D=1)。WER = (1+1)/3 = 66.7%。</details>
2.  **归一化陷阱**:
    *   为什么在评测前必须做“中文转阿拉伯数字”？
    *   **Hint**: 思考 "一千" 和 "1000" 的字面差异。
3.  **RTF 计算**:
    *   处理 10 秒音频耗时 1 秒，RTF 是多少？
    *   <details><summary>答案</summary>0.1。非常快。</details>

### 挑战题
4.  **AEC 评测设计**:
    *   你需要评估一套新的 AEC 算法。请设计一个包含“双讲（Double Talk）”场景的测试用例，并说明如何标注真值。
    *   **Hint**: 参考信号（音乐）+ 目标信号（人声）。真值是人声的内容。评估指标不仅是 WER，还有残余噪声等级。
5.  **流式修正的代价**:
    *   如果一个流式模型的最终 WER 很低，但 Flicker Rate 极高（用户看到的字一直在变），这在车载导航场景下会有什么安全隐患？
    *   **Hint**: 驾驶员分心。如果屏幕上的字一直在跳变，驾驶员会不自觉地盯着屏幕确认，导致视线离开路面。
6.  **多模态幻觉调试**:
    *   用户输入语音“这个怎么卖”，同时摄像头拍了一张苹果的照片。ASR 识别成了“这个怎么迈”。请问这是 ASR 的锅还是 MLLM 的锅？如何设计实验定位？
    *   **Hint**: 单独测 ASR 的输出。如果 ASR 输出确实是“迈”，那就是 ASR 错误。如果 ASR 输出是“卖”，但最终回答奇怪，那就是 MLLM 的图文对齐问题。

---

## 4.10 常见陷阱与错误 (Gotchas)

1.  **采样率不匹配**: 训练用 16k，测评用 8k 或 48k 重采样，导致性能断崖式下跌。**务必检查采样率一致性。**
2.  **时间戳错位**: 在计算 Latency 时，客户端录音时间和服务器接收时间存在网络抖动（Jitter）。**建议在音频中嵌入不可听的高频水印用于对齐，或基于本地录音回环测试。**
3.  **过拟合测试集**: 许多开源模型已经见过 LibriSpeech 的 Test-clean。**必须使用自建的、Out-of-domain 的数据（如车内实录）做最终验收。**
4.  **忽视 VAD 丢字**: 有时候 WER 高不是识别错了，而是 VAD 把头尾切掉了。**听一下 Badcase 的切片音频往往能发现真相。**
