# 第 9 章：人头/人脸图像与视频理解（AU、Blendshape、DMS/OMS）

## 1. 开篇段落

在多模态大模型（MLLM）的能力版图中，人脸与人头理解不仅仅是传统的“身份识别”（Face ID），更重要的是**状态理解**与**细粒度语义映射**。本章聚焦于模型如何从像素中通过面部动作单元（Action Units, AU）、3D 表情系数（Blendshapes）、视线（Gaze）和头姿（Pose）来推断人类的意图、情绪以及生理状态（如疲劳）。

对于驾舱一体化场景，这部分能力是 DMS（驾驶员监控系统）和 OMS（乘客监控系统）的核心。不同于传统的小模型检测方案，MLLM 的优势在于能够结合上下文（如“刚刚发生急刹车” + “驾驶员表情惊恐”）进行更合理的**语义推理**，而非简单的阈值触发。本章将详细拆解如何客观评测这些细粒度指标，以及如何设计鲁棒的实验来验证模型在极端光照和遮挡下的可靠性。

## 2. 测评体系论述

### 2.1 任务谱系与能力分层

人脸理解任务可以从几何、语义、状态三个维度进行分层。测评时需明确模型输出的是数值（回归任务）还是自然语言描述（推理任务）。

```ascii
+---------------------------------------------------------------+
|                 人脸/人头理解能力分层 (Hierarchy)               |
+---------------------------------------------------------------+
|  L3: 状态与意图 (State & Intent) [时序 + 上下文]                |
|      -> 疲劳分级 (Fatigue Level)                              |
|      -> 分心判定 (Distraction)                                |
|      -> 情绪推理 (Emotion Reasoning)                          |
+---------------------------------------------------------------+
|  L2: 语义编码 (Semantic Encoding) [帧级]                       |
|      -> Action Units (AU01, AU04...) - FACS 系统              |
|      -> 离散表情分类 (Happy, Sad, Neutral)                     |
+---------------------------------------------------------------+
|  L1: 几何特征 (Geometric Features) [像素级 -> 3D]              |
|      -> Blendshapes (52 coefficients ARKit standard)          |
|      -> Gaze Vector (视线向量 pitch/yaw)                       |
|      -> Head Pose (6DoF)                                      |
+---------------------------------------------------------------+
```

### 2.2 核心指标与打分方法

#### 2.2.1 Action Units (AU) 与表情
AU 是面部肌肉运动的原子单位。由于 AU 在自然场景下极度不平衡（如 AU12 嘴角上扬很常见，但 AU27 张嘴极大很少见），**Accuracy 是具有误导性的**。

*   **Rule-of-Thumb**: 必须使用 **F1-Score** 或 **AUC** 作为主要指标。对于 MLLM，如果输出是文本（如 "AU1, AU2 present"），需要解析文本后计算 F1。
*   **数据集**: BP4D, DISFA, EmotioNet。

#### 2.2.2 Blendshapes (BS)
Blendshapes 通常是一组 0.0 到 1.0 的浮点数系数（例如 ARKit 的 52 个基动）。这不仅用于驱动数字人，也是 MLLM 理解细微表情的量化输出。

*   **指标**: 平均绝对误差 (MAE) 或 均方误差 (MSE)。
*   **难点**: 不同的 3D 模型底座（Topology）定义的 BS 可能不同。评测时必须对齐到同一标准（通常推荐 ARKit 标准）。

#### 2.2.3 视线 (Gaze) 与头姿 (Pose)
*   **指标**: 角度误差（Angular Error，单位：度）。通常分别计算 Pitch（俯仰）和 Yaw（偏航）的误差，再求欧氏距离。
*   **数据集**: MPIIGaze, Gaze360, 300W-LP (Pose), AFLW2000-3D。

#### 2.2.4 DMS/OMS 状态判定（疲劳/分心）
这是时序视频理解任务。
*   **指标**:
    *   **事件级 Recall/Precision**: 在一段 1 分钟视频中，模型是否正确标记出了“闭眼 > 1.5秒”的时间段。
    *   **Time-to-Detect (TTD)**: 从疲劳特征出现到模型报警的延迟时间。

### 2.3 鲁棒性评测 (Robustness & Ablation)

人脸评测最容易受环境影响，必须在测评集中设计专门的 Ablation 子集：

1.  **光照与传感器**:
    *   **RGB vs IR (近红外)**: 车内常驻 IR 摄像头，模型在黑白/红外图上的表现通常与 RGB 差异巨大，需单独测。
    *   **极端光照**: 侧光（阴阳脸）、逆光（过曝）、低照度。
2.  **遮挡 (Occlusion)**:
    *   佩戴物：墨镜（透光 vs 不透光）、口罩、帽子。
    *   手部遮挡：手托腮、揉眼睛、喝水遮挡。
3.  **视角 (Viewpoint)**:
    *   大角度侧脸（Profile view）是很多 MLLM 的盲区。

### 2.4 训练数据反查与“以人脸为 Key”的过拟合

MLLM 常见的一个陷阱是**记住“人”而不是“表情”**。
*   **反查策略**: 在训练集和测试集中即使没有重叠的图片，如果存在**同一个人的不同图片**（Identity Leakage），测试分数也会虚高。
*   **对策**: 确保 Train/Test Split 是 **Subject-independent**（按人划分，而非按图划分）。

---

## 3. 本章小结

*   **几何到语义**: 评测应覆盖从底层的 Blendshape 回归到高层的 DMS 状态推理。
*   **指标陷阱**: AU 评测避免使用 Accuracy，使用 F1；连续变量使用 MAE。
*   **身份隔离**: 严格执行 Subject-independent 的数据集划分，防止模型通过记住 ID 来作弊。
*   **多模态价值**: MLLM 的核心价值在于处理 Long-tail 场景（如“司机虽然闭眼了，但因为他在打喷嚏，而不是睡觉”），评测应包含此类逻辑推理题。

---

## 4. 练习题

### 基础题
1.  **概念辨析**: 请简述 Action Unit (AU) 和 Blendshape 的区别。为什么在驱动 3D 数字人 avatar 时通常首选 Blendshape？
    <details><summary>Hint</summary>AU 基于解剖学肌肉运动（FACS），描述的是“发生了什么动作”；Blendshape 基于几何形变，描述的是“目标形状由哪些基底合成”。BS 直接对应 3D 渲染参数。</details>

2.  **指标计算**: 假设测试集中有 100 张人脸图，其中只有 5 张包含 AU27（张大嘴）。模型预测这 100 张全都没有 AU27。请问 Accuracy 是多少？F1-score 是多少？这说明了什么？
    <details><summary>Hint</summary>Accuracy = 95% (看似很高)；Recall = 0，Precision = 0，F1 = 0。说明 Accuracy 在类别极度不平衡时失效。</details>

3.  **光照鲁棒性**: 在车内环境中，为什么 RGB 模型在夜间几乎不可用？评测时应引入什么类型的图像数据？
    <details><summary>Hint</summary>车内夜间无自然光，需依赖主动红外补光。必须评测 IR (Infrared) / NIR (Near-Infrared) 灰度图像。</details>

4.  **视线追踪**: Gaze 评测中，"Eye contact"（眼神接触）通常如何定义？它与具体的 Pitch/Yaw 角度有何关系？
    <details><summary>Hint</summary>Eye contact 通常定义为视线向量落入摄像头（或特定目标区域）的一定角度锥体范围内（例如 5-10度误差内）。</details>

### 挑战题
5.  **DMS 逻辑推理**: 设计一个评测用例，用于区分“疲劳闭眼”和“有意识闭眼（如思考或听音乐）”。MLLM 需要结合哪些上下文信息？
    <details><summary>Hint</summary>需要结合：闭眼时长（微睡 vs 长闭眼）、头部动作（点头/垂头 vs 仰头）、手部动作、甚至语音输入（是否在哼歌）。</details>

6.  **Blendshape 跨域问题**: 你使用一个在真实人脸数据集（如 300W-LP）上微调的模型去预测二次元/卡通风格人脸的 Blendshape，结果发现误差很大。请分析原因并提出评测改进方案。
    <details><summary>Hint</summary>Domain Gap。真实人脸纹理与卡通差异大，且拓扑结构不同。评测应建立风格化人脸的专用测试集（可通过 3D 引擎渲染生成 Ground Truth）。</details>

7.  **隐私与评测**: 在构建 OMS（乘客监控）评测集时，如何处理 GDPR 或本地隐私法规带来的限制？如何证明模型在未见过的数据上依然有效？
    <details><summary>Hint</summary>使用合成数据（Synthetic Data）或经过严格脱敏（去标识化）的数据集。评测重点在于“动作/状态”而非“身份”。</details>

---

## 5. 常见陷阱与错误 (Gotchas)

| 陷阱类型 | 描述 | 调试/规避技巧 |
| :--- | :--- | :--- |
| **Landmark $\neq$ Blendshape** | 很多初学者试图直接用 2D 关键点（Landmarks）的距离来线性计算 Blendshape，导致非线性表情（如嘴唇卷曲）丢失。 | 必须使用 3D 标注数据或通过 3DMM 拟合获得 Ground Truth，不要手动写规则转换。 |
| **AU 共现幻觉** | 模型预测出解剖学上不可能并存的 AU 组合（如嘴唇同时极度上扬和下撇）。 | 在打分器中加入“解剖学约束检查（Anatomical Constraints Check）”，对不合理组合进行惩罚。 |
| **坐标系混淆** | 视线/头姿的 Pitch/Yaw/Roll 定义不一致（左手系 vs 右手系，相机坐标系 vs 世界坐标系）。 | **标准化预处理**：所有 Ground Truth 和预测结果在评测前统一转换到同一坐标系定义。 |
| **时间抖动** | 视频评测中，单帧预测准确，但连续帧之间数值跳变剧烈。 | 引入 **Temporal Smoothness** 指标（如相邻帧预测值的二阶差分），惩罚高频抖动。 |
| **人种/年龄偏差** | 模型在特定人种或儿童/老人脸上失效。 | 检查测试集的分布（Demographic bias），分桶汇报指标。 |

---

## 6. 车舱落地：驾舱一体专门讨论

在智能座舱中，人头/人脸理解是连接物理世界与数字交互的纽带。评测需特别关注以下**全链路交互**场景：

### 6.1 DMS/OMS 触发逻辑与对话融合
*   **评测场景**: 驾驶员打哈欠（DMS检测到）$\rightarrow$ MLLM 接收 Token $\rightarrow$ 语音助手主动询问：“检测到您有些疲劳，需要为您播放提神的音乐或导航到最近的服务区吗？”
*   **关键指标**:
    *   **端到端延迟**: 从哈欠动作结束到 TTS 声音响起的总耗时（建议 < 2秒）。
    *   **打扰率 (False Alarm Annoyance)**: 误报导致的主动打扰频率。如果用户只是在大笑被误判为哈欠，助手频繁打断会造成极差体验。
    *   **多模态拒绝**: 如果驾驶员戴墨镜，DMS 无法判断视线，MLLM 应当知道“数据缺失”并保持静默，而不是幻觉出视线方向。

### 6.2 多人多位置仲裁 (Multi-person Arbitration)
*   **OMS 场景**: 后排右侧乘客说“打开车窗”。
*   **技术链路**: 声纹定位（VAD/DOA） + OMS 视觉定位（唇动检测/Face ID）。
*   **评测重点**:
    *   **视听一致性 (Audio-Visual Sync)**: 只有当**说话人的嘴部动作**与**音频时间戳**对齐，且**位置**匹配时，才执行指令。
    *   **测试集构建**: 需要录制“一人说话，旁人张嘴不发声”或“旁人说话，目标闭嘴”的干扰样本，测试模型的抗干扰能力。

### 6.3 隐私与合规 (Privacy & Compliance)
车内摄像头属于高度敏感数据。
*   **评测红线**:
    *   **本地闭环**: 所有人脸图像必须在端侧（NPU）处理，评测需验证**没有任何图像数据**被上传到云端 MLLM（仅上传抽象后的语义 Token，如 `<state>drowsy</state>`）。
    *   **数据遗忘**: 每次熄火或行程结束，内存中的特征缓存是否被彻底清除。

### 6.4 极端场景与降级策略
*   **低光/遮挡兜底**: 当光照不足导致视觉失效时，MLLM 是否能平滑切换到纯语音交互模式，并给出合理的解释（“光线太暗，我看不清您的手势，请直接告诉我指令”）。
