# 第 8 章：视频理解（含人流、事件、时序推理、驾驶相关）

## 8.1 开篇：时间维度的引入——从“看图说话”到“观察思考”

在第 7 章中，我们解决了模型对静态切片的理解能力。然而，现实世界是连续流动的。视频理解不仅仅是图像理解的简单堆叠（Video $\neq$ Image $\times N$），它引入了 MLLM 评测中最具挑战性的维度：**时间动态性（Temporal Dynamics）**与**因果逻辑（Causal Logic）**。

本章将构建一套全面的视频理解测评体系，旨在回答以下核心问题：
1.  **时序感知**：模型是真正看懂了动作的发生顺序，还是仅仅通过单帧画面在“猜”？
2.  **长程记忆**：在长视频（分钟级以上）中，模型能否记住早期的细节并在后期进行关联？
3.  **高风险决策**：在驾驶场景下，模型能否准确识别稍纵即逝的险情（Corner Case）并给出合理解释？

我们将从通用的短视频描述，深入到安防监控（人流、异常事件），最终聚焦于车载环境下的自动驾驶（AD）与座舱感知场景。

## 8.2 视频理解能力金字塔

视频任务的难度是分层的，测评也应遵循此梯度：

```ascii
+-----------------------------------------------------------------------+
| Level 4: 预测与决策 (Prediction & Decision) [自动驾驶核心]            |
| 任务：轨迹预测、险情预判、驾驶规划解释                                |
| 示例："右侧行人有冲出来的趋势，建议立即减速。"                        |
+-----------------------------------------------------------------------+
| Level 3: 因果推理 (Causal Reasoning)                                  |
| 任务：解释动作原因、推断未展示的后果                                  |
| 示例："为什么这辆车突然变道？因为前方有施工障碍物。"                  |
+-----------------------------------------------------------------------+
| Level 2: 时序定位与计数 (Temporal Grounding & Counting)               |
| 任务：动作起止时间戳检测、特定动作计数                                |
| 示例："第15秒到20秒在切菜"、"视频中一共经过了3辆红色卡车"             |
+-----------------------------------------------------------------------+
| Level 1: 全局感知与分类 (Global Perception)                           |
| 任务：视频分类、通用描述 (Captioning)                                 |
| 示例："这是一个关于烹饪的视频"、"有人在打篮球"                        |
+-----------------------------------------------------------------------+
| Level 0: 静态帧识别 (Frame-level Recognition)                         |
| 任务：OCR、物体检测 (复用第7章能力)                                   |
| 示例："路牌上写着什么"、"画面里有几个人"                              |
+-----------------------------------------------------------------------+
```

## 8.3 数据集与测评基准选型

为了实现“及时全面”的测评，我们需要组合不同类型的公开数据集，并针对车载场景构建私有集。

### 8.3.1 开源基准地图

| 类别 | 数据集名称 | 特性与用途 | 推荐指数 |
| :--- | :--- | :--- | :--- |
| **综合能力 (SOTA)** | **MVBench** | 包含20个子任务（时序、空间、粗/细粒度），全自动评分。 | ⭐⭐⭐⭐⭐ (必测) |
| **通用问答** | **VideoChatGPT Bench** | 开放式问答，关注正确性、细节、上下文。需 LLM 打分。 | ⭐⭐⭐⭐ |
| **驾驶/交通** | **DriveLM / NuScenes-QA** | **车载核心**。基于真实驾驶数据的感知+推理+规划问答。 | ⭐⭐⭐⭐⭐ (必测) |
| **长视频理解** | **EgoSchema / MovieChat** | 长达几分钟至小时的视频，考察长时记忆（Long-term Memory）。 | ⭐⭐⭐ |
| **幻觉检测** | **VideoHallucer** | 专门诱导模型产生不存在的物体或动作。 | ⭐⭐⭐⭐ |
| **异常检测** | **UCSD Ped / Traffic-QA** | 包含人行道骑车、逆行等异常事件。适合安防/哨兵模式。 | ⭐⭐⭐ |
| **动作计数** | RepCount | 专门测试重复性动作（如跳绳、切菜）的计数能力。 | ⭐⭐ |

> **Rule of Thumb**: 开始评测时，**MVBench** 是目前性价比最高的“冒烟测试”工具；进入垂直领域优化时，必须基于 **DriveLM** 构建针对性的回归集。

## 8.4 关键测评技术与指标设计

### 8.4.1 客观题指标（MVBench 模式）
对于多项选择题，使用 **Accuracy (Acc)**。但为了深入分析，需计算**按任务分类的 Acc**：
*   $Acc_{Action}$: 动作识别准确率
*   $Acc_{Object}$: 物体存在性准确率
*   $Acc_{Order}$: 时序顺序判断准确率（如：先拿杯子还是先倒水？）

### 8.4.2 生成题指标与 Judge 设计
对于开放式描述（Captioning）或问答（QA），传统的 BLEU/ROUGE 指标与人类感知相关性极低。
**标准做法：LLM-as-a-Judge**。

*   **Prompt 模板设计**：
    ```text
    你是一个公正的视频分析专家。
    问题：{question}
    标准答案：{ground_truth}
    模型回答：{prediction}
    请从以下维度打分 (1-5)：
    1. 正确性 (Correctness)：是否包含关键信息？
    2. 幻觉 (Hallucination)：是否编造了不存在的物体？(如有，直接0分)
    3. 时序逻辑 (Temporal Logic)：动作顺序描述是否正确？
    输出格式：{"score": 4, "reason": "..."}
    ```
*   **Judge 模型选择**：GPT-4o 或专门微调过的 Video-Critic 模型。

### 8.4.3 时序定位指标 (Temporal Grounding)
针对“找出某个动作发生的时间段”任务：
*   **mIoU (mean Intersection over Union)**：预测时间段 $P$ 与真值 $G$ 的交集除以并集。
    *   公式：$\text{IoU} = \frac{\text{intersection}(P, G)}{\text{union}(P, G)}$
*   **Recall@1, IoU=0.5**：Top-1 预测中，IoU > 0.5 的比例。

### 8.4.4 驾驶专用指标
在驾驶场景（DriveLM）中，除文本相似度外，还需评估：
*   **规划一致性 (Planning Consistency)**：模型生成的驾驶建议（减速/左转）是否与真值（CAN Bus 信号）一致。
*   **关键对象召回 (Critical Object Recall)**：对红绿灯、横穿行人的提及率。**这是安全红线指标。**

## 8.5 自动化测评工程实现

### 8.5.1 采样策略（Sampling Strategy）：工程核心
视频无法全量输入 LLM（Context 爆炸），如何“降维”直接决定测评结果。

1.  **Uniform Sampling (均匀采样)**：最常用。将视频等分为 $N$ 段，每段取 1 帧。
    *   *推荐配置*：短视频 ($<15s$) 取 8-16 帧；长视频取 32-64 帧。
2.  **Keyframe-based (关键帧采样)**：利用 OpenCV 计算帧间差分或光流，只保留变化大的帧。
    *   *优势*：去除静止画面，节省 Token。
    *   *劣势*：可能丢失微小的关键运动。
3.  **FPS 对齐**：**常见陷阱！** 不同数据集 FPS 不同（电影 24，监控 10-30）。测评器必须将所有视频重采样到统一的时间轴，否则“第 10 帧”代表的时间点完全不同。

### 8.5.2 评测管线架构 (Pipeline)

```ascii
[Video File] 
    | -> (Decoder: ffmpeg/decord) -> [Raw Frames Cache] *性能瓶颈点*
    |
    +-> [Sampler] -> (Strategy: Uniform/Clip) -> [Selected Frames]
    |                                                 |
    +-> [Audio Extractor] -> (Whisper/CLAP) -> [Audio Features] (可选)
                                                      |
    [Question] + [Prompt] ------------------------> [VLM Input Generator]
                                                      |
                                                   [MLLM Inference]
                                                      |
    [Reference Answer] <--------------------------- [Model Output]
            |                                         |
            +-----------> [Scorer / Judge] <----------+
                              |
                       [Final Report]
```

## 8.6 Ablation 与 失败分析

当模型得分下降时，如何科学地“甩锅”？

### 8.6.1 必做的 Ablation 实验矩阵
1.  **Blind Test (盲测)**：
    *   *操作*：输入全黑视频，只给问题。
    *   *目的*：检查模型是否在靠语言偏见（Language Bias）答题。如果盲测准确率很高，说明数据集有严重 Bias。
2.  **Shuffle Test (乱序测试)**：
    *   *操作*：将输入的视频帧随机打乱顺序。
    *   *判据*：如果乱序后，时序类问题（如“车是前进了还是后退了”）得分**没有显著下降**，说明模型**根本没懂时序**，只是在看静态图。
3.  **Resolution Sweep (分辨率扫描)**：
    *   *操作*：对比 224px, 336px, 448px, 672px。
    *   *经验*：驾驶场景（看清远处的红绿灯）对分辨率极度敏感；动作识别（有人在跑步）对分辨率不敏感。

### 8.6.2 训练数据反查
*   **静态与动态失衡**：如果模型总是描述静态物体而忽略动作，反查训练数据中 Video-Text pair 的比例。通常视频数据应占 20%-30% 才能激发动态能力。
*   **字幕作弊**：WebVid 等数据集常包含硬字幕。检查模型是否只是在做 OCR。
    *   *验证*：对视频进行 Mask 遮挡字幕区域，看性能降幅。

## 8.7 车舱落地：驾舱一体专项测评

本节针对车载特殊环境，设计了一套端到端的测评方案。

### 8.7.1 场景一：行车记录仪/哨兵模式 (Sentinel Mode)
*   **用户需求**：“帮我看看昨晚有没有人动我的车。”
*   **技术难点**：长视频（8小时）+ 极稀疏事件（几秒异常）+ 隐私脱敏。
*   **测评用例设计**：
    1.  **正样本**：合成一段 1 小时视频，其中包含 10 秒有人靠近窥视。要求模型输出时间戳精度在 $\pm 5s$ 内。
    2.  **隐私合规**：视频中包含清晰人脸和车牌。要求生成的描述中自动脱敏（如：“检测到一名男性”，而非“检测到张三”），且不能输出 OCR 到的车牌号（除非用户授权）。
    3.  **低光照鲁棒性**：使用夜间/红外摄像头拍摄的数据集进行回归。

### 8.7.2 场景二：复杂路口与险情解释 (Driving Assistant)
*   **用户需求**：“刚才为什么突然刹车？”
*   **技术难点**：多视角融合（前视+侧视）、高动态、因果推理。
*   **DriveLM 测评集落地**：
    *   **Perception**: "左前方有什么车？"（必须召回遮挡严重但有碰撞风险的车辆）。
    *   **Prediction**: "这辆自行车接下来可能怎么走？"
    *   **Logic**: "请解释当前场景下减速的原因。"
*   **安全护栏 (Safety Guardrail)**：
    *   如果模型对红绿灯颜色判断错误，必须有一套传统的 CV 检测器作为 **Fallback** 进行纠正或置信度打分。测评时需统计 Fallback 触发率。

### 8.7.3 场景三：端侧与云侧协同 (Edge-Cloud Collaboration)
*   **架构**：端侧小模型（NPU）做筛选，云侧大模型（GPU）做推理。
*   **测评指标**：
    *   **端侧召回率**：端侧模型漏掉关键事件（False Negative）的代价是巨大的，必须 > 99%。
    *   **云侧幻觉率**：云侧模型不能对模糊的画面瞎编（如把塑料袋看成猫）。
    *   **全链路时延**：从用户提问 -> 视频切片上传 -> 云端推理 -> 语音下发。目标 < 3s (5G环境)。

---

## 8.8 本章小结

1.  **视频 $\neq$ 图片序列**：测评必须包含乱序测试（Shuffle Test），以验证模型是否具备真正的时序推理能力。
2.  **采样决定上限**：工程上，帧采样策略对性能的影响往往大于模型微调。务必针对长/短视频使用不同的采样参数。
3.  **驾驶场景特殊性**：DriveLM 类数据集是核心。除了考“看到了什么”，更要考“意味着什么（预测）”和“该怎么办（规划）”。
4.  **指标分层**：客观指标（Acc）看基础，主观指标（Judge）看体验，安全指标（Recall）看底线。

## 8.9 练习题

**基础题**
1.  **[计算]** 一段 1 分钟的视频，FPS=30。如果采用 `Uniform Sampling` 抽取 16 帧，请问采样间隔是多少帧？如果采用 1 FPS 的抽取率，最终输入多少帧？
2.  **[判断]** 为什么在评测 Video-LLM 时，直接使用 BLEU-4 分数来衡量生成的 Video Caption 是不推荐的？请列举两个原因。
3.  **[概念]** 解释什么是 "Temporal Grounding" 任务，并给出一个具体的车载应用场景。

**挑战题**
4.  **[设计]** 你需要设计一个“路怒症检测”的测评集。不仅要检测画面中的攻击性行为，还要结合音频（Audio）。请描述正负样本的构成，以及如何设计 Prompt 避免模型对正常大声说话产生误报。
5.  **[分析]** 在 NuScenes-QA 测评中，模型在白天场景表现优异，但在雨夜场景下对“行人”的召回率下降了 40%。除了增加雨夜训练数据外，从**预处理**和**多模态融合**的角度，你可以提出哪些改进方案并如何设计对应的 Ablation？
6.  **[思考]** 车载端侧芯片算力有限，无法运行 GPT-4V 级别的模型。请设计一个 **Cascade（级联）测评方案**，评估“端侧小模型过滤 + 云侧大模型精修”这一架构的综合性能（考虑精度、流量成本和延迟）。

<details>
<summary>点击查看提示 (Hint)</summary>

*   **题 1 提示**：间隔 = 总帧数 / 16。注意取整问题。
*   **题 2 提示**：1) 语义相同但词汇不同（"车停了" vs "车辆静止"）；2) 无法衡量逻辑因果。
*   **题 3 提示**：在时间轴上定位起止点。场景：用户说“把刚才差点撞车的视频剪辑出来”。
*   **题 4 提示**：正样本：画面有肢体冲突+声音高亢；负样本：车内KTV（声音大但在笑）。Prompt 需强调“情绪判断”和“威胁性评估”。
*   **题 5 提示**：预处理：伽马校正、去雨算法；多模态：引入雷达/激光雷达投影图作为额外输入通道。
*   **题 6 提示**：定义“关键帧召回率”作为端侧指标；定义“最终问答准确率”作为云侧指标；计算 (端侧上传帧数 * 流量单价 + 云侧推理成本) 作为经济指标。

</details>

## 8.10 常见陷阱与错误 (Gotchas)

*   **陷阱 1：视频解码器版本地狱**
    *   *问题*：`Decord` 和 `PyAV` 在处理变帧率（VFR）视频时，提取的帧索引可能不一致。
    *   *后果*：测评时的时间戳与 Ground Truth 对不上，导致 mIoU 极低。
    *   *对策*：在 Pipeline 初始化时，先运行一段校准脚本，确保 time-to-frame 的映射是单调且准确的。
*   **陷阱 2：内存泄漏 (OOM)**
    *   *问题*：视频 Tensor 极大，PyTorch 的显存经常爆。
    *   *对策*：不要把所有测试视频一次性加载到 RAM。实现一个 `LazyLoader`，测一个视频解压一个，测完立即释放。
*   **陷阱 3：忽视 Prompt 对时序的影响**
    *   *问题*：Prompt 写“描述这张图片” vs “描述这段视频”，模型输出差异巨大。
    *   *对策*：必须在 System Prompt 中显式强调“你正在观看一段视频，请注意时间流逝和动作变化”。
